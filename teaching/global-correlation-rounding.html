<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>global-correlation-rounding</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../styling.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="global-correlation-rounding">Global Correlation Rounding</h2>
<p><strong>Alert: these notes are a work in progress, and have not been subjected to the usual scrutiny reserved for formal publications!</strong></p>
<p>We will describe another approach for rounding pseudoexpectations. By contrast to the Gaussian rounding strategy used for max-cut, this time we will make use of higher-order moments of <span class="math inline">\(\tilde{\mathbb E}\)</span>. We will continue to use max-cut as our running example, but the ideas we will see are useful well beyond the setting of max-cut.</p>
<p>Recall that for an <span class="math inline">\(n\)</span>-vertex graph <span class="math inline">\(G\)</span>, we have defined <span class="math inline">\(G(x) = \sum_{i \sim j} (x_i - x_j)^2\)</span> to be the function which counts the number of edges of <span class="math inline">\(G\)</span> cut by some <span class="math inline">\(0/1\)</span> vector <span class="math inline">\(x\)</span>. Our first goal will be to prove the following theorem:</p>
<p><strong>Theorem 1 (Barak-Raghavendra-Steurer):</strong> For every <span class="math inline">\(n\)</span>-vertex graph <span class="math inline">\(G\)</span> and even <span class="math inline">\(d \in \mathbb{N}\)</span>, <span class="math inline">\(\vdash_d G(x) \leq \max_y G(y) + O(n^2 / \sqrt{d})\)</span>.</p>
<p>Here we have introduced a new notational shorthand: <span class="math inline">\(\vdash_d f \leq g\)</span> is shorthand for <span class="math inline">\(\vdash_d f - g \geq 0\)</span>.</p>
<p>Let us compare Theorem 1 to the theorem we previously proved for degree-2 SoS upper bounds on <span class="math inline">\(G\)</span>: we have traded the constant <span class="math inline">\(1/0.878\)</span> for the constant <span class="math inline">\(1\)</span>, at the cost of the additive term <span class="math inline">\(O(n^2 / \sqrt{d})\)</span>. When is this a good trade? Well, if <span class="math inline">\(G\)</span> is <em>dense</em> – i.e., has <span class="math inline">\(\Omega(n^2)\)</span> edges – then <span class="math inline">\(\max_y G(y) = \Theta(n^2)\)</span>. By choosing <span class="math inline">\(d = O(1/\varepsilon^2)\)</span> large enough that the additive error is at most <span class="math inline">\(\varepsilon n^2\)</span>, this shows that the SoS algorithm provides a <span class="math inline">\((1+O(\varepsilon))\)</span>-approximation to the max-cut value in dense graphs in <span class="math inline">\(n^{O(1/\epsilon^2)}\)</span> time.</p>
<p>(Once again, implicit in the proof will be an algorithm for actually finding such a cut, rather than just approximating the max-cut value.)</p>
<p>To avoid some technicalities, we will prove a slightly weaker result, which still implies a <span class="math inline">\((1+\epsilon)\)</span> approximation to max-cut in dense graphs in <span class="math inline">\(n^{\poly(1/\epsilon)}\)</span> time:</p>
<p><strong>Theorem 1’</strong> For every <span class="math inline">\(n\)</span>-vertex graph <span class="math inline">\(G\)</span> with <span class="math inline">\(\Omega(n^2)\)</span> edges and even <span class="math inline">\(d \in \mathbb{N}\)</span>, <span class="math inline">\(\vdash_d G(x) \leq \max_y G(y) + n^2 / d^{\Omega(1)}\)</span>.</p>
<h3 id="pseudoexpectations-local-distributions-and-conditioning">Pseudoexpectations, Local Distributions, and Conditioning</h3>
<p>First, we’ll lay a little groundwork, and in the process see another respect in which pseudoexpectations act like the moments of actual distributions on <span class="math inline">\(\{0,1\}^n\)</span>.</p>
<p><strong>Lemma 2 (Local Distributions):</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a pseudoexpectation of degree <span class="math inline">\(d\)</span>. For every <span class="math inline">\(|S| \subseteq [n]\)</span> with <span class="math inline">\(|S| \leq d/2\)</span>, there is a distribution <span class="math inline">\(\mu_S \, : \, \{0,1\}^k \rightarrow \mathbb{R}\)</span> such that for every <span class="math inline">\(T \subseteq S\)</span>, <span class="math inline">\(\tilde{\mathbb E}x^T = \mathbb E_{y \sim \mu_S} y^T\)</span>.</p>
<p><em>Proof:</em> Fixing <span class="math inline">\(S\)</span>, consider <span class="math inline">\(\tilde{\mathbb E}\)</span> restricted to the variables <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i \in S\)</span>; there are at most <span class="math inline">\(d/2\)</span> of them and <span class="math inline">\(\tilde{\mathbb E}\)</span> has degree <span class="math inline">\(d\)</span>. Therefore, <span class="math inline">\(\tilde{\mathbb E}\)</span> represents the moments of an actual distribution <span class="math inline">\(\mu_S\)</span> on <span class="math inline">\(\{0,1\}^{|S|}\)</span>. QED.</p>
<p>Another respect in which pseudoexpectations act like (moments of) distributions is that they can be <em>conditioned</em> on events (as long as those events are sufficiently simple, by which we mean that they can be described by a low-degree polynomial).</p>
<p>Concretely, let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a pseudoexpectation of degree <span class="math inline">\(d\)</span>, and let <span class="math inline">\(i \in [n]\)</span> be such that <span class="math inline">\(\tilde{\mathbb E}x_i &gt; 0\)</span>. If <span class="math inline">\(\tilde{\mathbb E}\)</span> were the moments of an actual distribution, <span class="math inline">\(x_i\)</span> would be a variable which has nonzero probability of being equal to <span class="math inline">\(1\)</span>, and we could condition on that event. To “pseudo-condition” on the event, we define a new pseudoexpectation:</p>
<p><span class="math display">\[\tilde{\mathbb E}[\cdot \, | \, x_i = 1] \, : \, \mathbb{R}[x]_{\leq d-2} \rightarrow \mathbb{R}\]</span> via</p>
<p><span class="math display">\[\tilde{\mathbb E}[ x^S \, | \, x_i = 1] := \frac{\tilde{\mathbb E}[x^S x_i]}{\tilde{\mathbb E}[x_i]}\]</span></p>
<p>Similarly, if <span class="math inline">\(\tilde{\mathbb E}[(1-x_i)] &gt; 0\)</span>, we can define</p>
<p><span class="math display">\[\tilde{\mathbb E}[ x^S \, | \, x_i = 0] := \frac{\tilde{\mathbb E}[x^S (1-x_i)]}{\tilde{\mathbb E}[(1-x_i)]}\]</span></p>
<p><strong>Lemma 3 (Conditioning):</strong> <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 1]\)</span> and <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 0]\)</span> are pseudoexpectations of degree <span class="math inline">\(d-2\)</span>.</p>
<p><em>Proof:</em> Exercise (just check the definition of a pseudoexpectation).</p>
<p>For some intuition, let us sanity check that <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 1]\)</span> “acts like” moments of a distribution where <span class="math inline">\(x_i\)</span> always assumes the value <span class="math inline">\(1\)</span>. For instance, it should be the case that for any polynomial <span class="math inline">\(p\)</span>, <span class="math inline">\(\tilde{\mathbb E}[p(x) x_i \, | \, x_i = 1] = \tilde{\mathbb E}[p(x) \, | \, x_i = 1]\)</span>. Luckily, it is easy to check that this is the case.</p>
<h3 id="independent-rounding">Independent Rounding</h3>
<p>To prove Theorem 1, we will describe an algorithm which takes a pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> of degree <span class="math inline">\(d\)</span> and finds <span class="math inline">\(y \in \{0,1\}^n\)</span> such that <span class="math inline">\(G(y) \geq \tilde{\mathbb E}G(x) - O(n^2/ \sqrt{d})\)</span>. As before, we will need a “rounding strategy” to do this.</p>
<p>Here’s an even simpler idea than the Gaussian rounding approach we tried before. The pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> specifies <span class="math inline">\(1\)</span>-wise marginal distributions for each coordinate, where coordinate <span class="math inline">\(i\)</span> is <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\tilde{\mathbb E}x_i\)</span> and otherwise <span class="math inline">\(0\)</span>. (Exercise: sanity check that <span class="math inline">\(\tilde{\mathbb E}x_i \in [0,1]\)</span> so that this is well defined.) We could simply sample each coordinate independently from according to these distributions. (This is not so dumb as it seems: in fact, many “randomized rounding” schemes for linear programs have this flavor and lead to nontrivial algorithms.)</p>
<p>Of course, the resulting random vector <span class="math inline">\(y\)</span> will not necessarily have the same higher-order correlations as <span class="math inline">\(\tilde{\mathbb E}\)</span> – that is, the joint distributions <span class="math inline">\((y_i,y_j)\)</span> will have independent coordinates, while the joint (2-local) distributions <span class="math inline">\(\mu_{ij}\)</span> coming from <span class="math inline">\(\tilde{\mathbb E}\)</span> need not be. (Expressing the same concept in terms of pseudoexpectation values rather than local distributions: we expect <span class="math inline">\(\mathbb Ey_i y_j = \mathbb Ey_i \mathbb Ey_j\)</span> and <span class="math inline">\(\tilde{\mathbb E}x_i x_j\)</span> to be different.)</p>
<p>But, for a thought experiment, let us imagine that we have gotten lucky, and the local distributions <span class="math inline">\(\mu_{ij}\)</span> are close to independent. Concretely, suppose</p>
<p><span class="math display">\[\sum_{i,j} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \leq \delta n^2.\]</span></p>
<p>Here, <span class="math inline">\(|\cdot |_{TV}\)</span> denotes total variation distance, and <span class="math inline">\(\mu_i \otimes \mu_j\)</span> is the product of the two <span class="math inline">\(1\)</span>-local distributions extracted from the pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span>. Let’s look at what happens when we use the independent rounding strategy on <span class="math inline">\(\tilde{\mathbb E}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
E_y G(y) &amp; = \sum_{i \sim j} \Pr(y_i \neq y_j)\\
&amp; = \sum_{i \sim j} \Pr_{x \sim \mu_i \otimes \mu_j}(x_i \neq x_j) \\
&amp; \geq \sum_{i \sim j} \Pr_{x \sim \mu_{ij}}(x_i \neq x_j) - |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \\
&amp; \geq \sum_{i \sim j} \Pr_{x \sim \mu_{ij}}(x_i \neq x_j) - \delta n^2 \\
&amp; = \tilde{\mathbb E}G(x) - \delta n^2.
\end{align*}\]</span></p>
<p>In this case (still a thought experiment), independent rounding has gone well, incurring just an additive <span class="math inline">\(\delta n^2\)</span> loss! But how can we arrange for <span class="math inline">\(\tilde{\mathbb E}\)</span> to satisfy the approximate 2-wise independence condition?</p>
<h3 id="reducing-global-correlation-by-conditioning">Reducing Global Correlation by Conditioning</h3>
<p>The following lemma captures the key idea we’ll use to arrange for <span class="math inline">\(\tilde{\mathbb E}\)</span> to satisfy the condition needed for independent rounding. It was discovered independently by Montanari, in the context of statistical physics and later by Barak, Raghavendra, Steurer, and Tan, in the SoS context. The name “pinning lemma” comes from the statistical physics literature, where the act of conditioning on an event <span class="math inline">\(x_i = 1\)</span> is referred to as “pinning” the value of <span class="math inline">\(x_i\)</span> to <span class="math inline">\(1\)</span>.</p>
<p><strong>Lemma 4 (Pinning Lemma):</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a degree-<span class="math inline">\(d\)</span> pseudoexpectation on <span class="math inline">\(n\)</span> variables, with <span class="math inline">\(d \ll n\)</span>. There exists <span class="math inline">\(t \leq d-2\)</span> such that if <span class="math inline">\(S \subseteq [n]\)</span> is a random set of coordinates with <span class="math inline">\(|S| = t\)</span> and <span class="math inline">\(y_S\)</span> is a sample from the local distribution <span class="math inline">\(\mu_T\)</span> induced on <span class="math inline">\(\{0,1\}^t\)</span> by <span class="math inline">\(\tilde{\mathbb E}\)</span>,</p>
<p><span class="math display">\[\mathbb E_{S,y_S} \sum_{i, j} | \mu_{ij \, | \, y_S} - \mu_{i \, | \, y_S} \otimes \mu_{j \, | \, y_S} |_{TV} \leq O(n^2 / \sqrt{d}),\]</span></p>
<p>where <span class="math inline">\(\mu_{T \, | \, y_S}\)</span> denotes the local distribution on <span class="math inline">\(T\)</span> of the conditional pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_S = y_S]\)</span> (which we define by conditioning iteratively on the coordinates).</p>
<p><em>Proof idea:</em> Before we prove the lemma, let’s explain the main idea, as usual drawing on intuition we cultivate by thinking of <span class="math inline">\(\tilde{\mathbb E}\)</span> as representing the moments of an actual distribution. Suppose we have such a distribution <span class="math inline">\(\mu\)</span>, and that the coordinates of <span class="math inline">\(\mu\)</span> are quite correlated, <span class="math inline">\(\sum_{i,j} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n^2\)</span>. We can rewrite this as</p>
<p><span class="math display">\[ \mathbb E_{i \sim [n]} \sum_{j \leq n} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n,\]</span></p>
<p>so for a randomly chosen coordinate <span class="math inline">\(i\)</span>, we expect to have <span class="math inline">\(\sum_{j \leq n} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n\)</span> – the <span class="math inline">\(i\)</span>-th coordinate is nontrivially correlated with lots of other coordinates <span class="math inline">\(j\)</span>. Intuitively, if you learned the value of the <span class="math inline">\(i\)</span>-th coordinate in a sample from <span class="math inline">\(\mu\)</span>, you would therefore also learn a lot about the values of the other coordinates!</p>
<p>A little more formally, conditioning on the value of coordinate <span class="math inline">\(i\)</span> should cause the <em>entropy</em> of the other coordinates to decrease. There is only so much entropy available, meaning that if we repeat this conditioning procedure, eventually we should have <span class="math inline">\(\sum_{i,j} |\mu&#39;_{ij} - \mu&#39;_i \otimes \mu&#39;_j|_{TV} \leq \delta n^2\)</span>, where <span class="math inline">\(\mu&#39;\)</span> is some conditioning of <span class="math inline">\(\mu\)</span> on the values of some coordinates. It turns out that this will happen after at most <span class="math inline">\(O(1/\delta^2)\)</span> conditioning steps.</p>
<p>The cleanest way to express these ideas formally goes via entropy and mutual information. There are many excellent resources to learn elementary information theory, so we’ll just state the basic facts we need to use here.</p>
<p>Let <span class="math inline">\(H(X)\)</span> denote the entropy of a (discrete) random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(I(X;Y)\)</span> the mutual information between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p><strong>Lemma (Entropy Facts):</strong></p>
<ol type="1">
<li>If <span class="math inline">\(X,Y\)</span> are jointly-distributed <span class="math inline">\(\{0,1\}\)</span>-valued random variables, <span class="math inline">\(H(X),H(Y),I(X;Y) \in [0,1]\)</span>.</li>
<li>(Pinsker’s inequality) For jointly distributed random variables <span class="math inline">\(X,Y\)</span>, denote by <span class="math inline">\(\mu\)</span> the joint distribution and <span class="math inline">\(\mu_X, \mu_Y\)</span> the marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Then <span class="math inline">\(|\mu - \mu_X \otimes \mu_Y|_{TV} \leq \sqrt{\frac 12 I(X;Y)}\)</span>.</li>
</ol>
<p><em>Proof of Lemma 4:</em> For <span class="math inline">\(s \leq d-2\)</span>, let us define a (random) sequence of pseudoexpectations <span class="math inline">\(\tilde{\mathbb E}= \tilde{\mathbb E}^{(0)},\tilde{\mathbb E}^{(1)},\ldots,\tilde{\mathbb E}^{(d-2)}\)</span>, where <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span> is obtained from <span class="math inline">\(\tilde{\mathbb E}^{(s-1)}\)</span> by choosing a random <span class="math inline">\(i \in [n]\)</span> which has not been chosen in a previous step, sampling <span class="math inline">\(y_i\)</span> from the local distribution <span class="math inline">\(\mu^{(s-1)}_i\)</span>, and setting <span class="math inline">\(\tilde{\mathbb E}^{(s)} = \tilde{\mathbb E}^{(s-1)}[ \cdot \, | \, x_i = y_i]\)</span>.</p>
<p>(Exercise: check that the following alternative definition gives the same distribution for <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span>. Choose <span class="math inline">\(S \subseteq [n]\)</span> with <span class="math inline">\(|S| = s\)</span> at random and sample <span class="math inline">\(y_S\)</span> from the local distribution <span class="math inline">\(\mu_S\)</span> induced by <span class="math inline">\(\tilde{\mathbb E}\)</span>. Then let <span class="math inline">\(\tilde{\mathbb E}^{(s)} = \tilde{\mathbb E}[\cdot \, | \, x_S = y_S]\)</span>.)</p>
<p>Consider the <em>global information</em> for the <span class="math inline">\(s\)</span>-th pseudoexpectation:</p>
<p><span class="math display">\[\text{global}_s := \mathbb E_{i,j \in [n]} I(X_i^{(s)}; X_j^{(s)})\]</span></p>
<p>where, in the <span class="math inline">\(i,j\)</span>-th term of the sum, <span class="math inline">\(X_i^{(s)},X_j^{(s)}\)</span> denote a jointly distributed sample from the <span class="math inline">\(2\)</span>-local distribution <span class="math inline">\(\mu^{(s)}_{ij}\)</span> induced by <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span>.</p>
<p>If, for some <span class="math inline">\(s\)</span>, we have <span class="math inline">\(\mathbb E\text{global}_s \leq \delta\)</span>, then by convexity of <span class="math inline">\(\sqrt{\cdot}\)</span> and Pinsker’s inequality, we have <span class="math inline">\(\mathbb E\sum_{i,j} |\mu_{ij}^{(s)} - \mu_i^{(s)} \otimes \mu_j^{(s)}|_{TV} \leq \sqrt{\delta} n^2\)</span>. For <span class="math inline">\(\delta = O(1/d)\)</span>, this would complete the proof, so we just need to show that <span class="math inline">\(\mathbb E\text{global}_s \leq O(1 / d)\)</span> for some <span class="math inline">\(s \leq d-2\)</span>.</p>
<p>We introduce the following potential function tracking the amount of entropy in <span class="math inline">\(1\)</span>-local distributions: <span class="math display">\[\phi^{(s)} := \mathbb E\mathbb E_{i \in [n]}  H(X_i^{(s)}).\]</span> (The outer expectation averages over all the randomness in the choice of <span class="math inline">\(s\)</span> and the values to be conditioned on.) The key claim, proved below, is that for <span class="math inline">\(d \ll n\)</span>,: <span class="math display">\[\phi^{(s)} - \phi^{(s+1)} \leq \Omega(\mathbb E\text{global}_{s}).\]</span></p>
<p>We know <span class="math inline">\(0 \leq \phi^{(s)} \leq 1\)</span>, using <span class="math inline">\(H(X) \in [0,1]\)</span> for a binary random variable. So, <span class="math display">\[1 \geq \phi^{(0)} - \phi^{(d-2)} \geq \sum_{s \leq d-2} \mathbb E\text{global}_s.\]</span> Finally, since mutual information is nonnegative, <span class="math inline">\(\text{global}_s \geq 0\)</span> for all <span class="math inline">\(s\)</span>. So, some term in the sum is at most <span class="math inline">\(O(1/d)\)</span>. QED</p>
<p><em>Proof of claim:</em> By definition, <span class="math inline">\(I(X_i^{(s-1)};X_j^{(s-1)}) = H(X_i^{(s-1)}) - H(X_i^{(s-1)} \, | \, X_j^{(s-1)})\)</span>. In the <span class="math inline">\(s\)</span>-th step, we choose some index <span class="math inline">\(j_s\)</span> to condition on, having already conditioned on indices <span class="math inline">\(j_1,\ldots,j_{s-1}\)</span>. So, <span class="math display">\[\begin{align*}
\phi^{(s-1)} - \phi^{(s)} &amp; = \mathbb E_{j_1,\ldots,j_{s-1}} \mathbb E_{j \sim [n]\setminus j_1,\ldots,j_{s-1}} \mathbb E_{i \sim [n]} H(X_i^{(s-1)} - H(X_i^{(s-1)} | X_j^{(s-1)}) \\
&amp; = \mathbb E_{j_1,\ldots,j_{s-1}} \mathbb E_{i \sim [n] \setminus j_1,\ldots,j_{s-1}} I(X_i^{(s-1)}; X_j^{(s-1)}) \\
&amp; \leq \frac n {n-(s-1)} \mathbb E\text{global}_{s-1}.
\end{align*}\]</span> QED.</p>
<h3 id="putting-it-together">Putting it together</h3>
<p>Now we’re ready to prove Theorem 1.</p>
<p><em>Proof of Theorem 1</em> Suppose <span class="math inline">\(\tilde{\mathbb E}\)</span> is a degree-<span class="math inline">\(d\)</span> pseudoexpectation. Following the procedure in Lemma 4, we know that we can obtain a (random) conditional pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span>, with local distributions <span class="math inline">\(\mu&#39;\)</span> satisfying <span class="math display">\[
\mathbb E\sum_{i,j \leq n} |\mu&#39;_{ij} - \mu_i&#39; \otimes \mu_j&#39;|_{TV} \leq O(n^2/\sqrt{d}).
\]</span> Furthermore, <span class="math inline">\(\mathbb E\tilde{\mathbb E}&#39; G(x) = \tilde{\mathbb E}G(x)\)</span>, so if we view <span class="math inline">\(\tilde{\mathbb E}&#39; G(x)\)</span> itself as a random variable bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(n^2\)</span>, we have <span class="math inline">\(\Pr( \tilde{\mathbb E}&#39; G(x) &lt; (1-\alpha) \tilde{\mathbb E}G(x)) \leq \alpha \tilde{\mathbb E}G(x) / n^2\)</span>, for any <span class="math inline">\(\alpha &gt; 0\)</span>. By Markov’s inequality, we can assume the local distributions <span class="math inline">\(\mu&#39;\)</span> satisfy <span class="math display">\[
\sum_{i,j \leq n} |\mu&#39;_{ij} - \mu_i&#39; \otimes \mu_j&#39;|_{TV} \leq \frac 1 \alpha \cdot O(n^2/\sqrt{d})
\]</span> with probability <span class="math inline">\(1-\alpha\)</span>. Choosing <span class="math inline">\(\alpha = 1/d^{1/4}\)</span>, there is <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span> with local distributions <span class="math inline">\(\mu&#39;\)</span> such that <span class="math display">\[
\sum_{i,j \leq n} |\mu&#39;_{ij} - \mu_i&#39; \otimes \mu_j&#39;|_{TV} \leq O(n^2/d^{1/4})
\]</span> and <span class="math inline">\(\tilde{\mathbb E}&#39; G(x) \geq (1-d^{-1/4}) \tilde{\mathbb E}G(x)\)</span>. Putting this together with our previous analysis of independent rounding, we can see that independent rounding produces a cut <span class="math inline">\(y\)</span> with <span class="math inline">\(E G(y) \geq \tilde{\mathbb E}G(x) - O(n^2 / d^{1/4})\)</span>. QED.</p>
<h2 id="local-to-global-extending-to-expanders">Local-to-Global: Extending to Expanders</h2>
<p>It’s possible to take these ideas well beyond Theorem 1 – generalizing to constraint satisfaction problems beyond max-cut, as well as past the case of dense graphs. We’ll show one important generalization here, to <em>expander graphs</em>.</p>
<p>For this section, we consider <span class="math inline">\(\Delta\)</span>-regular graphs on <span class="math inline">\(n\)</span> vertices. Let <span class="math inline">\(A_G\)</span> be the normalized adjacency matrix of such a graph <span class="math inline">\(G\)</span>, so <span class="math inline">\(\lambda_1(A) = 1\)</span>.</p>
<p><strong>Theorem 2:</strong> For every <span class="math inline">\(\Delta\)</span>-regular graph <span class="math inline">\(G\)</span> and every even <span class="math inline">\(d\)</span>, <span class="math inline">\(\vdash_d G(x) \leq (1 + d^{-\Omega(1)} + \lambda_2^{\Omega(1)})\max_{y} G(y)\)</span>.</p>
<p>In the case of dense graphs, we could show that independent rounding succeeded when the quantity <span class="math display">\[
(1) \qquad \mathbb E_{i,j \sim [n]} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV}
\]</span> was small. In sparse graphs, this is no longer the case. But, by inspecting the foregoing analysis, we can see that independent rounding will work fine if we can bound <span class="math display">\[
(2) \qquad \mathbb E_{i\sim j} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV},
\]</span> that is, measuring how close the <span class="math inline">\(2\)</span>-local distributions are to being product when averaged only over edges in <span class="math inline">\(G\)</span>.</p>
<p>For certain graphs – expanders among them – it is possible to show that the pinning operation actually leads to a pseudoexpectation for which (2) is small, via a “local to global” lemma. We’ll need a definition first – here it is convenient to switch to the <span class="math inline">\(\{\pm 1\}\)</span> basis for the hypercube, which is done implicitly in the definition statement.</p>
<p><strong>Definition (Global and Local Correlation)</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a degree-<span class="math inline">\(2\)</span> pseudoexpectation, and for each <span class="math inline">\(i \in [n]\)</span> let <span class="math inline">\(y_i = y_i(x) = 2x_i - 1\)</span>. Let <span class="math inline">\(Cov_{ij} = \tilde{\mathbb E}y_i y_j - \tilde{\mathbb E}y_i \tilde{\mathbb E}y_j\)</span>. We define the <em>global correlation</em> as <span class="math display">\[
\text{global-corr} = \mathbb E_{i,j \sim [n]} Cov_{ij}^2
\]</span> and, for a graph <span class="math inline">\(G\)</span>, the <em>local correlation</em> as <span class="math display">\[
\text{local-corr} = \mathbb E_{i \sim j} Cov_{ij}^2.
\]</span></p>
<p>The following lemma relates covariance to information (and is the reason we switched from the <span class="math inline">\(x\)</span>’s to the <span class="math inline">\(y\)</span>’s).</p>
<p><strong>Lemma (Covariance versus Information):</strong> For <span class="math inline">\(\{ \pm 1\}\)</span>-valued random variables <span class="math inline">\(X,Y\)</span>, <span class="math inline">\(Cov(X,Y)^2 \leq O(I(X;Y))\)</span>.</p>
<p><em>Proof:</em> See <a href="https://arxiv.org/pdf/1110.1064.pdf">Raghavendra-Tan</a>, Fact B.5.</p>
<p><strong>Lemma (Local-to-Global):</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a degree-<span class="math inline">\(2\)</span> pseudoexpectation, <span class="math inline">\(G\)</span> a <span class="math inline">\(\Delta\)</span>-regular graph with normalized adjacency matrix <span class="math inline">\(A_G\)</span> whose eigenvalues are <span class="math inline">\(1 = \lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n\)</span>. Then <span class="math inline">\(\text{local-corr} \leq \text{global-corr} + O(\lambda_2)\)</span>.</p>
<p>In the process of proving the local-to-global lemma, we’ll see a new kind of SoS proof, built out of the eigenvalues/eigenvectors of a PSD matrix. The following argument will use some basic spectral graph theory – <a href="http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf">Dan Spielman’s book</a> is an excellent reference for this material.</p>
<p><em>Proof of local-to-global lemma:</em> Let us recall a few basic facts about the quadratic form <span class="math inline">\(v^\top A_G v\)</span>. Since <span class="math inline">\(A_G\)</span> is regular, its top eigenvector is the all-<span class="math inline">\(1\)</span>’s, so we can write <span class="math display">\[\begin{align*}
\frac 1 {\Delta} \sum_{i \sim j} v_i v_j &amp; = v^\top A_G v\\
&amp; =  \frac 1 n \sum_{i,j} v_i v_j + \sum_{1 &lt; j \leq n} \lambda_j(A_G) \langle v,w_j \rangle^2
\end{align*}\]</span> where <span class="math inline">\(w_j\)</span> is (unit-norm) the eigenvector of <span class="math inline">\(A_G\)</span> associated to eigenvalue <span class="math inline">\(\lambda_j(A_G)\)</span>.</p>
<p>Now let’s write local correlation as <span class="math display">\[\begin{align*}
\text{local-corr} &amp; = \mathbb E_{i \sim j} (\tilde{\mathbb E}y_i y_j - \tilde{\mathbb E}y_i \tilde{\mathbb E}y_j)^2 \\
&amp; = \frac 1 n \cdot \frac 1 \Delta \sum_{i \sim j} (\tilde{\mathbb E}y_i y_j - \tilde{\mathbb E}y_i \tilde{\mathbb E}y_j)^2 \\
&amp; = \frac 1 n \cdot \frac 1 \Delta \tilde{\mathbb E}\left [ \sum_{i \sim j}  ( y_i - \tilde{\mathbb E}y_i)(y_j - \tilde{\mathbb E}y_j) (y_i&#39; - \tilde{\mathbb E}y_i&#39;)(y_j&#39; - \tilde{\mathbb E}y_j) \right ]
\end{align*}\]</span> where <span class="math inline">\(y_i&#39;\)</span> and <span class="math inline">\(y_j&#39;\)</span> are independent copies of <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span>. (Exercise: check that there is a well-defined pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> on variables <span class="math inline">\(y_1,\ldots,y_n,y_1&#39;,\ldots,y_n&#39;\)</span>.)</p>
<p>The last expression we can rewrite again as <span class="math display">\[\begin{align*}
&amp; \frac 1 n \cdot \frac 1 \Delta \tilde{\mathbb E}\left [ \sum_{i \sim j}  ( y_i - \tilde{\mathbb E}y_i)(y_j - \tilde{\mathbb E}y_j) (y_i&#39; - \tilde{\mathbb E}y_i&#39;)(y_j&#39; - \tilde{\mathbb E}y_j) \right ]\\
&amp; \qquad = \frac 1 {n^2} \tilde{\mathbb E}\sum_{i,j \in [n]} ( y_i - \tilde{\mathbb E}y_i)(y_j - \tilde{\mathbb E}y_j) (y_i&#39; - \tilde{\mathbb E}y_i&#39;)(y_j&#39; - \tilde{\mathbb E}y_j) + \tilde{\mathbb E}\frac 1 n \sum_{1 &lt; j \leq n} \lambda_j(A_G) \langle v(y,y&#39;),w_j \rangle^2\\
&amp; \qquad = \text{global-corr} + \tilde{\mathbb E}\frac 1 n \sum_{1 &lt; j \leq n} \lambda_j(A_G) \langle v(y,y&#39;),w_j \rangle^2
\end{align*}\]</span> where <span class="math inline">\(v(y,y&#39;)\)</span> is the vector-valued polynomial given by <span class="math inline">\(v(y,y&#39;)_i = (y_i - \tilde{\mathbb E}y_i)(y_i&#39; - \tilde{\mathbb E}y_i&#39;)\)</span>.</p>
<p>Finally, we’ll use the following claim, established below: <span class="math display">\[
\vdash_2 \frac 1 n \sum_{1 &lt; j \leq n} \lambda_j(A_G) \langle v(y,y&#39;),w_j \rangle^2 \leq O(\lambda_2).
\]</span> All together, we’ve obtained <span class="math inline">\(\text{local-corr} \leq \text{global-corr} + O(\lambda_2)\)</span>.</p>
<p>Now let’s put things together to prove Theorem 2.</p>
<p><em>Proof sketch of Theorem 2:</em> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a degree-<span class="math inline">\(d\)</span> pseudoexpectation. Following the proof of Theorem 1’, we can assume there is a conditioning <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span> of <span class="math inline">\(\tilde{\mathbb E}\)</span>, with local distributions <span class="math inline">\(\mu&#39;\)</span>, such that:</p>
<ul>
<li><span class="math inline">\(\mathbb E_{i,j \in [n]} I(X_i&#39;,X_j&#39;) \leq d^{-\Omega(1)}\)</span></li>
<li><span class="math inline">\(\tilde{\mathbb E}&#39; G(x) \geq (1-d^{-\Omega(1)}) \tilde{\mathbb E}G(x).\)</span></li>
</ul>
<p>The first point implies that <span class="math inline">\(\mathbb E_{i,j \in [n]} Cov_{ij}^2 \leq d^{-\Omega(1)}\)</span>, so by the local-to-global lemma, <span class="math inline">\(\mathbb E_{i \sim j} Cov_{ij}^2 \leq d^{-\Omega(1)} + O(\lambda_2)\)</span>.</p>
<p>Exercise: show that <span class="math inline">\(|\mu&#39;_{ij} - \mu&#39;_i \otimes \mu&#39;_j|_{TV} \leq Cov_{ij}^{\Omega(1)}\)</span>.</p>
<p>Given this exercise, independent rounding produces a cut <span class="math inline">\(y\)</span> such that <span class="math inline">\(\mathbb EG(y) \geq (1-poly(1/d,\lambda_2)) \tilde{\mathbb E}G(x)\)</span>, which is what we wanted to show.</p>
</body>
</html>
