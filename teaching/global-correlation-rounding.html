<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>global-correlation-rounding</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../styling.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="global-correlation-rounding">Global Correlation Rounding</h2>
<p><strong>Alert: these notes are a work in progress!</strong></p>
<p>We will describe another approach for rounding pseudoexpectations. By contrast to the Gaussian rounding strategy used for max-cut, this time we will make use of higher-order moments of <span class="math inline">\(\tilde{\mathbb E}\)</span>. We will continue to use max-cut as our running example, but the ideas we will see are useful well beyond the setting of max-cut.</p>
<p>Recall that for an <span class="math inline">\(n\)</span>-vertex graph <span class="math inline">\(G\)</span>, we have defined <span class="math inline">\(G(x) = \sum_{i \sim j} (x_i - x_j)^2\)</span> to be the function which counts the number of edges of <span class="math inline">\(G\)</span> cut by some <span class="math inline">\(0/1\)</span> vector <span class="math inline">\(x\)</span>. Our first goal will be to prove the following theorem:</p>
<p><strong>Theorem 1 (Barak-Raghavendra-Steurer):</strong> For every <span class="math inline">\(n\)</span>-vertex graph <span class="math inline">\(G\)</span> and even <span class="math inline">\(d \in \mathbb{N}\)</span>, <span class="math inline">\(\vdash_d G(x) \leq \max_y G(y) + O(n^2 / \sqrt{d})\)</span>.</p>
<p>Here we have introduced a new notational shorthand: <span class="math inline">\(\vdash_d f \leq g\)</span> is shorthand for <span class="math inline">\(\vdash_d f - g \geq 0\)</span>.</p>
<p>Let us compare Theorem 1 to the theorem we previously proved for degree-2 SoS upper bounds on <span class="math inline">\(G\)</span>: we have traded the constant <span class="math inline">\(1/0.878\)</span> for the constant <span class="math inline">\(1\)</span>, at the cost of the additive term <span class="math inline">\(O(n^2 / \sqrt{d})\)</span>. When is this a good trade? Well, if <span class="math inline">\(G\)</span> is <em>dense</em> – i.e., has <span class="math inline">\(\Omega(n^2)\)</span> edges – then <span class="math inline">\(\max_y G(y) = \Theta(n^2)\)</span>. By choosing <span class="math inline">\(d = O(1/\varepsilon^2)\)</span> large enough that the additive error is at most <span class="math inline">\(\varepsilon n^2\)</span>, this shows that the SoS algorithm provides a <span class="math inline">\((1+O(\varepsilon))\)</span>-approximation to the max-cut value in dense graphs in <span class="math inline">\(n^{O(1/\epsilon^2)}\)</span> time.</p>
<p>(Once again, implicit in the proof will be an algorithm for actually finding such a cut, rather than just approximating the max-cut value.)</p>
<h3 id="pseudoexpectations-local-distributions-and-conditioning">Pseudoexpectations, Local Distributions, and Conditioning</h3>
<p>First, we’ll lay a little groundwork, and in the process see another respect in which pseudoexpectations act like the moments of actual distributions on <span class="math inline">\(\{0,1\}^n\)</span>.</p>
<p><strong>Lemma 2 (Local Distributions):</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a pseudoexpectation of degree <span class="math inline">\(d\)</span>. For every <span class="math inline">\(|S| \subseteq [n]\)</span> with <span class="math inline">\(|S| \leq d/2\)</span>, there is a distribution <span class="math inline">\(\mu_S \, : \, \{0,1\}^k \rightarrow \mathbb{R}\)</span> such that for every <span class="math inline">\(T \subseteq S\)</span>, <span class="math inline">\(\tilde{\mathbb E}x^T = \mathbb E_{y \sim \mu_S} y^T\)</span>.</p>
<p><em>Proof:</em> Fixing <span class="math inline">\(S\)</span>, consider <span class="math inline">\(\tilde{\mathbb E}\)</span> restricted to the variables <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i \in S\)</span>; there are at most <span class="math inline">\(d/2\)</span> of them and <span class="math inline">\(\tilde{\mathbb E}\)</span> has degree <span class="math inline">\(d\)</span>. Therefore, <span class="math inline">\(\tilde{\mathbb E}\)</span> represents the moments of an actual distribution <span class="math inline">\(\mu_S\)</span> on <span class="math inline">\(\{0,1\}^{|S|}\)</span>. QED.</p>
<p>Another respect in which pseudoexpectations act like (moments of) distributions is that they can be <em>conditioned</em> on events (as long as those events are sufficiently simple, by which we mean that they can be described by a low-degree polynomial).</p>
<p>Concretely, let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a pseudoexpectation of degree <span class="math inline">\(d\)</span>, and let <span class="math inline">\(i \in [n]\)</span> be such that <span class="math inline">\(\tilde{\mathbb E}x_i &gt; 0\)</span>. If <span class="math inline">\(\tilde{\mathbb E}\)</span> were the moments of an actual distribution, <span class="math inline">\(x_i\)</span> would be a variable which has nonzero probability of being equal to <span class="math inline">\(1\)</span>, and we could condition on that event. To “pseudo-condition” on the event, we define a new pseudoexpectation:</p>
<p><span class="math display">\[\tilde{\mathbb E}[\cdot \, | \, x_i = 1] \, : \, \mathbb{R}[x]_{\leq d-2} \rightarrow \mathbb{R}\]</span> via</p>
<p><span class="math display">\[\tilde{\mathbb E}[ x^S \, | \, x_i = 1] := \frac{\tilde{\mathbb E}[x^S x_i]}{\tilde{\mathbb E}[x_i]}\]</span></p>
<p>Similarly, if <span class="math inline">\(\tilde{\mathbb E}[(1-x_i)] &gt; 0\)</span>, we can define</p>
<p><span class="math display">\[\tilde{\mathbb E}[ x^S \, | \, x_i = 0] := \frac{\tilde{\mathbb E}[x^S (1-x_i)]}{\tilde{\mathbb E}[(1-x_i)]}\]</span></p>
<p><strong>Lemma 3 (Conditioning):</strong> <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 1]\)</span> and <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 0]\)</span> are pseudoexpectations of degree <span class="math inline">\(d-2\)</span>.</p>
<p><em>Proof:</em> Exercise (just check the definition of a pseudoexpectation).</p>
<p>For some intuition, let us sanity check that <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_i = 1]\)</span> “acts like” moments of a distribution where <span class="math inline">\(x_i\)</span> always assumes the value <span class="math inline">\(1\)</span>. For instance, it should be the case that for any polynomial <span class="math inline">\(p\)</span>, <span class="math inline">\(\tilde{\mathbb E}[p(x) x_i \, | \, x_i = 1] = \tilde{\mathbb E}[p(x) \, | \, x_i = 1]\)</span>. Luckily, it is easy to check that this is the case.</p>
<h3 id="independent-rounding">Independent Rounding</h3>
<p>To prove Theorem 1, we will describe an algorithm which takes a pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> of degree <span class="math inline">\(d\)</span> and finds <span class="math inline">\(y \in \{0,1\}^n\)</span> such that <span class="math inline">\(G(y) \geq \tilde{\mathbb E}G(x) - O(n^2/ \sqrt{d})\)</span>. As before, we will need a “rounding strategy” to do this.</p>
<p>Consider the following idea, even simpler than the Gaussian rounding approach we tried before. The pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> specifies <span class="math inline">\(1\)</span>-wise marginal distributions for each coordinate, where coordinate <span class="math inline">\(i\)</span> is <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\tilde{\mathbb E}x_i\)</span> and otherwise <span class="math inline">\(0\)</span>. (Exercise: sanity check that <span class="math inline">\(\tilde{\mathbb E}x_i \in [0,1]\)</span> so that this is well defined.) We could simply sample each coordinate independently from according to these distributions. (This is not so dumb as it seems: in fact, many “randomized rounding” schemes for linear programs have this flavor and lead to nontrivial algorithms.)</p>
<p>Of course, the resulting random vector <span class="math inline">\(y\)</span> will not necessarily have the same higher-order correlations as <span class="math inline">\(\tilde{\mathbb E}\)</span> – that is, the joint distributions <span class="math inline">\((y_i,y_j)\)</span> will have independent coordinates, while the joint (2-local) distributions <span class="math inline">\(\mu_{ij}\)</span> coming from <span class="math inline">\(\tilde{\mathbb E}\)</span> need not be. (Expressing the same concept in terms of pseudoexpectation values rather than local distributions: we expect <span class="math inline">\(\mathbb Ey_i y_j = \mathbb Ey_i \mathbb Ey_j\)</span> and <span class="math inline">\(\tilde{\mathbb E}x_i x_j\)</span> to be different.)</p>
<p>But, for a thought experiment, let us imagine that we have gotten lucky, and the local distributions <span class="math inline">\(\mu_{ij}\)</span> are close to independent. Concretely, suppose</p>
<p><span class="math display">\[\sum_{i,j} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \leq \delta n^2.\]</span></p>
<p>Here, <span class="math inline">\(|\cdot |_{TV}\)</span> denotes total variation distance, and <span class="math inline">\(\mu_i \otimes \mu_j\)</span> is the product of the two <span class="math inline">\(1\)</span>-local distributions extracted from the pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span>. Let’s look at what happens when we use the independent rounding strategy on <span class="math inline">\(\tilde{\mathbb E}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
E_y G(y) &amp; = \sum_{i \sim j} \Pr(y_i \neq y_j)\\
&amp; = \sum_{i \sim j} \Pr_{x \sim \mu_i \otimes \mu_j}(x_i \neq x_j) \\
&amp; \geq \sum_{i \sim j} \Pr_{x \sim \mu_{ij}}(x_i \neq x_j) - |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \\
&amp; \geq \sum_{i \sim j} \Pr_{x \sim \mu_{ij}}(x_i \neq x_j) - \delta n^2 \\
&amp; = \tilde{\mathbb E}G(x) - \delta n^2.
\end{align*}\]</span></p>
<p>In this case (still a thought experiment), independent rounding has gone well, incurring just an additive <span class="math inline">\(\delta n^2\)</span> loss! But how can we arrange for <span class="math inline">\(\tilde{\mathbb E}\)</span> to satisfy the approximate 2-wise independence condition?</p>
<h3 id="reducing-global-correlation-by-conditioning">Reducing Global Correlation by Conditioning</h3>
<p>The following lemma captures the key idea we’ll use to arrange for <span class="math inline">\(\tilde{\mathbb E}\)</span> to satisfy the condition needed for independent rounding. It was discovered independently by Montanari, in the context of statistical physics and later by Barak, Raghavendra, Steurer, and Tan, in the SoS context. The name “pinning lemma” comes from the statistical physics literature, where the act of conditioning on an event <span class="math inline">\(x_i = 1\)</span> is referred to as “pinning” the value of <span class="math inline">\(x_i\)</span> to <span class="math inline">\(1\)</span>.</p>
<p><strong>Lemma 4 (Pinning Lemma):</strong> Let <span class="math inline">\(\tilde{\mathbb E}\)</span> be a degree-<span class="math inline">\(d\)</span> pseudoexpectation. There exists <span class="math inline">\(t \leq d-2\)</span> such that if <span class="math inline">\(S \subseteq [n]\)</span> is a random set of coordinates with <span class="math inline">\(|S| = t\)</span> and <span class="math inline">\(y_S\)</span> is a sample from the local distribution <span class="math inline">\(\mu_T\)</span> induced on <span class="math inline">\(\{0,1\}^t\)</span> by <span class="math inline">\(\tilde{\mathbb E}\)</span>,</p>
<p><span class="math display">\[\mathbb E_{S,y_S} \sum_{i, j} | \mu_{ij \, | \, y_S} - \mu_{i \, | \, y_S} \otimes \mu_{j \, | \, y_S} |_{TV} \leq O(n^2 / \sqrt{d}),\]</span></p>
<p>where <span class="math inline">\(\mu_{T \, | \, y_S}\)</span> denotes the local distribution on <span class="math inline">\(T\)</span> of the conditional pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}[\cdot \, | \, x_S = y_S]\)</span> (which we define by conditioning iteratively on the coordinates).</p>
<p><em>Proof idea:</em> Before we prove the lemma, let’s explain the main idea, as usual drawing on intuition we cultivate by thinking of <span class="math inline">\(\tilde{\mathbb E}\)</span> as representing the moments of an actual distribution. Suppose we have such a distribution <span class="math inline">\(\mu\)</span>, and that the coordinates of <span class="math inline">\(\mu\)</span> are quite correlated, <span class="math inline">\(\sum_{i,j} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n^2\)</span>. We can rewrite this as</p>
<p><span class="math display">\[ \mathbb E_{i \sim [n]} \sum_{j \leq n} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n,\]</span></p>
<p>so for a randomly chosen coordinate <span class="math inline">\(i\)</span>, we expect to have <span class="math inline">\(\sum_{j \leq n} |\mu_{ij} - \mu_i \otimes \mu_j|_{TV} \gg \delta n\)</span> – the <span class="math inline">\(i\)</span>-th coordinate is nontrivially correlated with lots of other coordinates <span class="math inline">\(j\)</span>. Intuitively, if you learned the value of the <span class="math inline">\(i\)</span>-th coordinate in a sample from <span class="math inline">\(\mu\)</span>, you would therefore also learn a lot about the values of the other coordinates!</p>
<p>A little more formally, conditioning on the value of coordinate <span class="math inline">\(i\)</span> should cause the <em>entropy</em> of the other coordinates to decrease. There is only so much entropy available, meaning that if we repeat this conditioning procedure, eventually we should have <span class="math inline">\(\sum_{i,j} |\mu&#39;_{ij} - \mu&#39;_i \otimes \mu&#39;_j|_{TV} \leq \delta n^2\)</span>, where <span class="math inline">\(\mu&#39;\)</span> is some conditioning of <span class="math inline">\(\mu\)</span> on the values of some coordinates. It turns out that this will happen after at most <span class="math inline">\(O(1/\delta^2)\)</span> conditioning steps.</p>
<p>The cleanest way to express these ideas formally goes via entropy and mutual information. There are many excellent resources to learn elementary information theory, so we’ll just state the basic facts we need to use here.</p>
<p>Let <span class="math inline">\(H(X)\)</span> denote the entropy of a (discrete) random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(I(X;Y)\)</span> the mutual information between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p><strong>Lemma (Entropy Facts):</strong></p>
<ol type="1">
<li>If <span class="math inline">\(X\)</span> is a <span class="math inline">\(\{0,1\}\)</span>-valued random variable, <span class="math inline">\(H(X) \in [0,1]\)</span>.</li>
<li>(Pinsker’s inequality): For jointly distributed random variables <span class="math inline">\(X,Y\)</span>, denote by <span class="math inline">\(\mu\)</span> the joint distribution and <span class="math inline">\(\mu_X, \mu_Y\)</span> the marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Then <span class="math inline">\(|\mu - \mu_X \otimes \mu_Y|_{TV} \leq \sqrt{\frac 12 I(X;Y)}\)</span>.</li>
</ol>
<p><em>Proof of Lemma 4:</em> For <span class="math inline">\(s \leq d-2\)</span>, let us define a (random) sequence of pseudoexpectations <span class="math inline">\(\tilde{\mathbb E}= \tilde{\mathbb E}^{(0)},\tilde{\mathbb E}^{(1)},\ldots,\tilde{\mathbb E}^{(d-2)}\)</span>, where <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span> is obtained from <span class="math inline">\(\tilde{\mathbb E}^{(s-1)}\)</span> by choosing a random <span class="math inline">\(i \in [n]\)</span> which has not been chosen in a previous step, sampling <span class="math inline">\(y_i\)</span> from the local distribution <span class="math inline">\(\mu^{(s-1)}_i\)</span>, and setting <span class="math inline">\(\tilde{\mathbb E}^{(s)} = \tilde{\mathbb E}^{(s-1)}[ \cdot \, | \, x_i = y_i]\)</span>.</p>
<p>(Exercise: check that the following alternative definition gives the same distribution for <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span>. Choose <span class="math inline">\(S \subseteq [n]\)</span> with <span class="math inline">\(|S| = s\)</span> at random and sample <span class="math inline">\(y_S\)</span> from the local distribution <span class="math inline">\(\mu_S\)</span> induced by <span class="math inline">\(\tilde{\mathbb E}\)</span>. Then let <span class="math inline">\(\tilde{\mathbb E}^{(s)} = \tilde{\mathbb E}[\cdot \, | \, x_S = y_S]\)</span>.)</p>
<p>Consider the <em>global information</em> for the <span class="math inline">\(s\)</span>-th pseudoexpectation: <span class="math display">\[\text{global}_s := \sum_{i,j} I(X_i,X_j)\]</span> where, in the <span class="math inline">\(i,j\)</span>-th term of the sum, <span class="math inline">\(X_i,X_j\)</span> denote a jointly distributed sample from the <span class="math inline">\(2\)</span>-local distribution <span class="math inline">\(\mu^{(s)}_{ij}\)</span> induced by <span class="math inline">\(\tilde{\mathbb E}^{(s)}\)</span>.</p>
<p>If, for some <span class="math inline">\(s\)</span>, we have <span class="math inline">\(\mathbb E\text{global}_s \leq \delta n^2\)</span>, then by convexity of <span class="math inline">\(\sqrt{\cdot}\)</span> and Pinsker’s inequality, we have <span class="math inline">\(\mathbb E\sum_{i,j} |\mu_{ij}^{(s)} - \mu_i^{(s)} \otimes \mu_j^{(s)}|_{TV} \leq \sqrt{\delta} n^2\)</span>. For <span class="math inline">\(\delta = \delta(d)\)</span> we’ll choose later, suppose that for every <span class="math inline">\(s \leq d-2\)</span>, we have <span class="math inline">\(\mathbb E\text{global}_s &gt; \delta n^2\)</span>.</p>
</body>
</html>
