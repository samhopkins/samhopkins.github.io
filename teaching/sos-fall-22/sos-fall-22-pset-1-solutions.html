<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>sos-fall-22-pset-1-solutions</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../../styling.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="problem-set-1-solutions">Problem Set 1 – Solutions</h2>
<p><strong>Problem 1A:</strong> Let <span class="math inline">\(q_1,\ldots,q_m\)</span> witness <span class="math inline">\(\vdash_d f \geq 0\)</span>, meaning that <span class="math inline">\(f(x) = \sum_{i \leq m} q_i(x)^2\)</span> for all <span class="math inline">\(x \in \{0,1\}^n\)</span>. Without loss of generality, we can assume that each <span class="math inline">\(q_i\)</span> is a multilinear polynomial of degree at most <span class="math inline">\(d\)</span>, and that <span class="math inline">\(m \leq n^d\)</span>. By hypothesis, <span class="math inline">\(\mathbb E_{x \sim \{0,1\}^n} f(x) \leq n^{O(d)}\)</span>, hence also <span class="math inline">\(\mathbb E_{x \sim \{0,1\}^n} q(x)^2 \leq n^{O(d)}\)</span> for each <span class="math inline">\(q = q_i\)</span>.</p>
<p>Now it’s convenient to view <span class="math inline">\(q\)</span> as a function over the <span class="math inline">\(\{\pm 1\}^n\)</span> cube. Let <span class="math inline">\(x \, : \, \{-1,1\}^n \rightarrow \{0,1\}^n\)</span> be given by <span class="math inline">\(x(y)_i = (y_i + 1)/2\)</span>, and let <span class="math inline">\(h(y) = g(x(y))\)</span>. By the same reasoning as above, <span class="math inline">\(\mathbb E_{y \sim \{ \pm 1\}^n} h(y)^2 \leq n^{O(d)}\)</span>. Consider the expansion of <span class="math inline">\(h\)</span> as <span class="math inline">\(h(y) = \sum_{\alpha \subseteq [n], |\alpha| \leq d} \hat{h}(\alpha) y^\alpha\)</span>, and observe that <span class="math inline">\(\mathbb E_{y \sim \{\pm 1\}^n} h(y)^2 = \sum_{\alpha} \hat{h}(\alpha)^2\)</span>. So we can conclude that <span class="math inline">\(|\hat{h}(\alpha)| \leq n^{O(d)}\)</span> for each <span class="math inline">\(\alpha\)</span>.</p>
<p>We just need to check that this transfers over to <span class="math inline">\(q\)</span>. So let’s write <span class="math inline">\(h\)</span> as a function over the <span class="math inline">\(\{0,1\}\)</span> cube; let <span class="math inline">\(y(x)_i = 2x_i - 1\)</span>; we have <span class="math inline">\(h(y(x)) = g(x)\)</span>. Expanding <span class="math inline">\(h\)</span> again, <span class="math inline">\(h(y(x)) = \sum_{\alpha} \hat{h}(\alpha) \prod_{i \in \alpha} (2x_i - 1)\)</span>. If we expanded the products here, we would see that each <span class="math inline">\(x^\beta\)</span> appears with coefficient at most <span class="math inline">\(2^d \sum_{\alpha} |\hat{h}(\alpha)| \leq n^{O(d)}\)</span>. QED.</p>
<p><strong>Problem 1B:</strong> Fix <span class="math inline">\(f\)</span>. We claim that the following convex set <span class="math inline">\(\mathcal{C}\subseteq \mathbb{R}^{\binom{n}{\leq d} \times \binom{n}{\leq d}}\)</span> contains a ball of radius <span class="math inline">\(\geq \varepsilon/ n^{O(d)}\)</span> and is contained in a ball of radius <span class="math inline">\(n^{O(d)}\)</span>. <span class="math display">\[\mathcal{C}= \{ M \, : \, M \succeq - (\varepsilon/n^{O(d)}) I \text{ and for all $\alpha \subseteq [n]$ } \widehat{(x^{\otimes \leq d})^\top M (x^{\otimes \leq d})}(\alpha) \in [\hat{f}(\alpha) - \varepsilon, \hat{f}(\alpha) + \varepsilon], \|M\|_F^2 \leq n^{O(d)} \}.\]</span> Here, <span class="math inline">\(\|M\|_F\)</span> is the Frobenius norm of <span class="math inline">\(M\)</span>. Clearly <span class="math inline">\(\mathcal{C}\)</span> is contained in a ball of radius <span class="math inline">\(R = n^{O(d)}\)</span>. To see that it contains a ball of radius <span class="math inline">\(\varepsilon/ n^{O(d)}\)</span>, consider the matrix given by <span class="math inline">\(M = \sum p_i p_i^\top\)</span>, where <span class="math inline">\(p_i \in \mathbb{R}^{\binom{n}{\leq d}}\)</span> is such that <span class="math inline">\(p_i(x) = \langle p_i, x^{\otimes \leq d} \rangle\)</span>, and <span class="math inline">\(p_1,\ldots,p_m\)</span> are an SoS proof of <span class="math inline">\(f \geq 0\)</span> such that <span class="math inline">\(\|p_i\|^2 \leq n^{O(d)}\)</span>. Then <span class="math inline">\(\mathcal{C}\)</span> contains a ball of radius <span class="math inline">\(\varepsilon/ n^{O(d)}\)</span> around <span class="math inline">\(M\)</span>. Constructing a separation oracle for <span class="math inline">\(\mathcal{C}\)</span> is easy by combining the separation oracle for PSD matrices with a simple algorithm separating over the linear constraints <span class="math inline">\(\widehat{(x^{\otimes \leq d})^\top M (x^{\otimes \leq d})}(\alpha) \in [\hat{f}(\alpha) - \varepsilon, \hat{f}(\alpha) + \varepsilon]\)</span>.</p>
<p>So, Ellipsoid will find some matrix <span class="math inline">\(M \in \mathcal{C}\)</span>, within the desired running time. Let <span class="math inline">\(q_1,\ldots,q_m\)</span> be a rank-one factorization of <span class="math inline">\(M + \varepsilon/n^{O(d)} I\)</span>; i.e. <span class="math inline">\(M+\varepsilon/n^{O(d)} I = \sum q_i q_i^\top\)</span> – we can find <span class="math inline">\(q_1,\ldots,q_m\)</span> in polynomial time, up to error <span class="math inline">\(\varepsilon&#39;\)</span>; that is, given <span class="math inline">\(M\)</span> we can find <span class="math inline">\(\tilde{q_i}\)</span> such <span class="math inline">\(\|M + \varepsilon/ n^{O(d)} - \sum q_i q_i^\top\|_{\infty} \leq \varepsilon&#39;\)</span>.</p>
<p>Then the coefficients of the polynomial <span class="math inline">\(g(x) = \sum \langle x^{\otimes \leq d}, q_i \rangle - f(x)\)</span> are at most <span class="math inline">\(n^{O(d)} \varepsilon&#39; + n^{-\Omega(d)} \varepsilon\)</span>. Expanding <span class="math inline">\(g(x) = \sum_{|\alpha| \leq d} \hat{g}(\alpha) x^{\alpha}\)</span>, we can decompose each <span class="math inline">\(\alpha = \alpha_0 \cup \alpha_1\)</span>, where <span class="math inline">\(|\alpha_0| = |\alpha_1| \pm 1\)</span>, and obtain <span class="math inline">\(x^{\alpha} \preceq x^{2\alpha_0} + x^{2 \alpha_1} = x^{\alpha_0} + x^{\alpha_1}\)</span>. Then, <span class="math inline">\(\vdash_d x^{\alpha_0} \leq 1\)</span>, since <span class="math inline">\(1-x^{\alpha_0} = 1-2x^{\alpha_0} + x^{2\alpha_0} = (1-x^{\alpha_0})^2\)</span>; we have constructed a degree-<span class="math inline">\(d\)</span> proof of <span class="math inline">\(x^{\alpha} \leq 1\)</span>. There is a similar proof for <span class="math inline">\(-x^{\alpha}\)</span>. By summing up these proofs, in polynomial time we can find a degree-<span class="math inline">\(d\)</span> proof of <span class="math inline">\(g(x) \leq \varepsilon/2 + \varepsilon&#39; n^{O(d)}\)</span> (choosing constants appropriately). Putting this proof together with the <span class="math inline">\(q_i\)</span>’s and taking <span class="math inline">\(\varepsilon&#39; \ll n^{-O(d)}\)</span> gives the proof we want for <span class="math inline">\(f + \varepsilon\)</span>.</p>
<p><strong>Problem 2:</strong> The algorithm is:</p>
<ul>
<li>find a degree-2 pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}\)</span> over the <span class="math inline">\(\{ \pm 1\}\)</span> hypercube such that <span class="math inline">\(\tilde{\mathbb E}G(x) = \tilde{\mathbb E}\sum_{i \sim j} \tfrac 1 4 (x_i - x_j)^2 \geq (1-\varepsilon)|E|\)</span></li>
<li>compute the centered pseudoexpectation <span class="math inline">\(\tilde{\mathbb E}&#39;p(x) = \tfrac 12 \tilde{\mathbb E}p(x) + \tfrac 12 \tilde{\mathbb E}p(-x)\)</span> (obtaining a list of <span class="math inline">\(n^2\)</span> numbers); note that <span class="math inline">\(\tilde{\mathbb E}&#39; x_i = 0\)</span> for all <span class="math inline">\(x_i\)</span>, while <span class="math inline">\(\tilde{\mathbb E}&#39; G(x) = \tilde{\mathbb E}G(x)\)</span>.</li>
<li>draw a sample <span class="math inline">\(g\)</span> from a Gaussian whose mean and covariance match that of <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span></li>
<li>let <span class="math inline">\(y_i = sign(g_i)\)</span>. Output the cut <span class="math inline">\(y\)</span>.</li>
</ul>
<p>To analyze the algorithm, we consider what happens along a single edge <span class="math inline">\(i \sim j\)</span>. Let <span class="math inline">\(\varepsilon_{ij} = 1- \frac 1 4 \tilde{\mathbb E}&#39; (x_i - x_j)^2\)</span>. By definition, <span class="math inline">\(\varepsilon_{ij} = \tfrac 12 + \tfrac 12 \tilde{\mathbb E}x_i x_j\)</span>, i.e. <span class="math inline">\(\tilde{\mathbb E}x_i x_j = -1 + 2 \varepsilon_{ij}\)</span>.</p>
<p>Let <span class="math inline">\(g,h\)</span> be variance-1 Gaussians with <span class="math inline">\(\mathbb Egh = -1 + 2 \varepsilon_{ij}\)</span>. We can sample <span class="math inline">\(g,h\)</span> jointly as follows. First, sample <span class="math inline">\(g \sim \mathcal{N}(0,1)\)</span>, also sample <span class="math inline">\(z \sim \mathcal{N}(0,1)\)</span>, and let <span class="math inline">\(h = (-1 + 2 \varepsilon_{ij}) \cdot g + 2 \sqrt{\varepsilon_{ij}(1-\varepsilon_{ij})} \cdot z\)</span>. To have <span class="math inline">\(sign(h) = sign(g)\)</span>, we would have to have <span class="math inline">\(|z| \geq \tfrac 1 {\Omega(\sqrt{\varepsilon_{ij}})} |g|\)</span>. We have</p>
<p><span class="math display">\[\Pr(|z| \geq \tfrac 1 {\Omega(\sqrt{\varepsilon_{ij}})} |g|) \leq \Pr(|z| \geq \sqrt{\log(1/\varepsilon_{ij}})) + \Pr(|g| \leq O(\sqrt{\varepsilon_{ij} \log 1/\varepsilon_{ij}})) \leq O(\sqrt{\varepsilon_{ij} \log 1/\varepsilon_{ij}})\]</span></p>
<p>So, <span class="math display">\[E_y G(y) \geq |E| - \sum_{i \sim j} \Pr( sign(g_i) = sign(g_j) ) \geq |E| - \sum_{i \sim j} O(\sqrt{\varepsilon_{ij} \log(1/\varepsilon{ij})}).\]</span> By concavity of <span class="math inline">\(\sqrt{x \log(1/x)}\)</span>, <span class="math inline">\(\mathbb E_{i\sim j} \sqrt{\varepsilon_{ij} \log(1/\varepsilon_{ij})} \leq \sqrt{ \mathbb E_{i \sim j} \varepsilon_{ij} \log 1/\mathbb E_{i \sim j} \varepsilon_{ij} } = \sqrt{\varepsilon\log(1/\varepsilon)}\)</span>. QED.</p>
<p><strong>Problem 3A:</strong> We check the property after one round of conditioning; then the rest follows by induction. Consider, WLOG, <span class="math inline">\(\tilde{\mathbb E}[ \cdot \, | \, x_1 = 1]\)</span>. We have <span class="math display">\[\tilde{\mathbb E}[ p(x) \sum_{i=1}^n x_i \, | \, x_i = 1] = \frac{\tilde{\mathbb E}p(x) \sum_{i=1}^n x_i}{\tilde{\mathbb E}x_1 } = \frac n 2 \frac{\tilde{\mathbb E}p(x) }{\tilde{\mathbb E}x_1} = \frac n 2 \tilde{\mathbb E}[ p(x) \, | \, x_1 = 1]\]</span></p>
<p><strong>Problem 3B:</strong> The variance is actually <span class="math inline">\(O(n)\)</span> – since the <span class="math inline">\(y_i\)</span>’s are independent, we have <span class="math inline">\(Var (\sum y_i) = \sum Var(y_i) \leq O(n)\)</span>.</p>
<p><strong>Problem 3C (sketch):</strong> The algorithm is:</p>
<ul>
<li>Given <span class="math inline">\(G\)</span>, find <span class="math inline">\(\tilde{\mathbb E}\)</span> of degree <span class="math inline">\((1/\varepsilon)^{O(1)}\)</span> maximizing <span class="math inline">\(G(x)\)</span> among balanced <span class="math inline">\(\tilde{\mathbb E}\)</span>’s.</li>
<li>By brute-force search over conditionings of <span class="math inline">\((1/\varepsilon)^{O(1)}\)</span> variables, find a conditioned <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span> such that <span class="math inline">\(\tilde{\mathbb E}&#39; G(x) \geq (1 - \varepsilon) \tilde{\mathbb E}G(x)\)</span> having global information <span class="math inline">\(\leq \varepsilon^5\)</span> (which exists by the same argument we used in lecture).</li>
<li>Apply independent rounding to <span class="math inline">\(\tilde{\mathbb E}&#39;\)</span>. With high probability, we obtain <span class="math inline">\(y\)</span> such that <span class="math inline">\(\sum y_i = n/2 \pm O(\sqrt n)\)</span>, using independence and a Chernoff bound. We can move any <span class="math inline">\(O(\sqrt{n})\)</span> nodes from one side of the cut to the other to balance it, losing at most <span class="math inline">\(n^{3/2}\)</span> cut edges in the process.</li>
</ul>
</body>
</html>
