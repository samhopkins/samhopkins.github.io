---
title: Sub-Gaussian Mean Estimation in Polynomial Time
tags: presentation
output:
  includes:
    in_header: header.html
---

(loading loading loading -- advance slide)

$\newcommand{\R}{\mathbb{R}}$
$\newcommand{\e}{\varepsilon}$
$\newcommand{\cD}{\mathcal{D}}$
$\newcommand{\poly}{\text{poly}}$
$\newcommand{\cN}{\mathcal{N}}$
$\newcommand{\tensor}{\otimes}$
$\newcommand{\E}{\mathop{\mathbb{E}}}$
$\renewcommand{\hat}{\widehat}$
$\newcommand{\iprod}[1]{\langle #1 \rangle}$
$\newcommand{\pE}{\tilde{\mathbb{E}}}$
$\newcommand{\Paren}[1]{\left ( #1 \right )}$

---

# Sub-Gaussian Mean Estimation in Polynomial Time

[Sam Hopkins (Miller Fellow, UC Berkeley)]{.center .blue}


---

## This talk is about estimating the mean of a random vector

. . .

**Input:** $X_1,\ldots,X_n \in \R^d$ independent copies of $X$

**Output:** estimator $\hat{\mu}(X_1,\ldots,X_n)$ of $\mu = \E X$.


**SoS is [r]{.magenta}[e]{.blue}[a]{.magenta}[l]{.blue}[l]{.magenta}[y]{.blue} [g]{.magenta}[r]{.blue}[e]{.magenta}[a]{.blue}[t]{.magenta}** (Thor's hammer, Sauron's ring, etc.)

. . .

Offers a method to design *efficient algorithms* based on *simple identifiability proofs*.

. . .

>The "usual" identifiability argument for **mean estimation** with $\e$-corrupted samples under **bounded covariance assumptions** with **$O(\sqrt \e)$ error** is the right kind of simple.\
>[Uses only Cauchy-Schwarz, triangle inequalities, hence has an SoS certificate]{.magenta .fragment}

. . .

**The holy trinity:** [simple identifiability proofs]{.blue}, [sum-of-squares polynomials]{.magenta}, [efficient algorithms]{.cyan}

---

Yesterday we saw:

If $X = \{x_1,\ldots,x_m\}$ are $\e$-corrupted from a distribution $\cD$ with mean $\mu$, variance $1$, whp there are degree $O(1)$ polynomials $s_i(w,X',\mu',g), q_j(w,X',\mu',g)$ such that

[$O(\e) - (\mu - \mu')^2 = \sum s_i^2 + \sum q_j p_j$]{.blue}

. . .

where $p_1=0,\ldots,p_m=0$ enforce

[$w_i^2 = w_i$ and $\sum w_i = (1-\e)m$]{.magenta}

[$w_i(X_i - X_i')$]{.magenta}

[$\mu' = \frac 1 {(1-\e)m} \sum w_i X_i'$]{.magenta}

[$\sum w_i(X_i - \mu')^2 + g^2 =1$]{.magenta}

**This is an SoS identifiability proof**

---

Not going to go through the basic identifiability proof yet again, but just a taste:

To *SoS-ify* the Cauchy-Schwarz step, use that

. . .

$\Paren{\sum_{i \leq n} y_i^2}\Paren{\sum_{i \leq n} x_i^2} -  \Paren{\sum_{i \leq n} x_i y_i}^2 = \sum_{i,j} (x_iy_j - x_j y_i)^2$


---


SoS identifiability proof + meta-theorem $\rightarrow$ efficient algorithm

>If $p_1(\hat{\Theta},W)=0,\ldots,p_m(\hat{\Theta},W)=0$ imply $\|\Theta - \hat{\Theta}\|^2 \leq \delta$ and this has an SoS proof of degree $t$, then there is an $(mn)^{O(t)}$ time algorithm to output $\Theta'$ with $\|\Theta' - \Theta\|^2 \leq \delta$.\
[$\delta - \|\Theta - \hat{\Theta}\|^2 = \sum s_i^2 + \sum q_j p_j$]{.blue .center .fragment}

. . .

**Proof of meta-theorem:**

. . .

Suppose linear operator $\pE \, : \, \R[\hat{\Theta},W]_{\leq t} \rightarrow \R$ such that

1. $\pE 1 = 1$
2. $\pE p^2 \geq 0$ for all $p$ such that $\deg p^2 \leq t$
3. $\pE p_i q = 0$ for all $q, p_i$ such that $\deg p_i q \leq t$

. . .

[Then $\pE \|\Theta - \hat{\Theta}\|^2 \leq \delta$, expands to $\|\Theta\|^2 + \pE \|\hat{\Theta}\|^2 - 2 \iprod{\Theta,\pE \hat{\Theta}} \leq \delta$.]{.blue}

. . .

[Since $\pE \|\hat{\Theta}\|^2 \geq \|\pE \hat{\Theta}\|^2$, we find $\|\pE \hat{\Theta} - \Theta\|^2 \leq \delta$.]{.blue}

---

Have some $p_1(y),\ldots,p_m(y)$

Suppose linear operator $\pE \, : \, \R[y]_{\leq t} \rightarrow \R$ such that

1. $\pE 1 = 1$
2. $\pE p^2 \geq 0$ for all $p$ such that $\deg p^2 \leq t$
3. $\pE p_i q = 0$ for all $q, p_i$ such that $\deg p_i q \leq t$

. . .

Set of such $\pE$ is feasible set of following SDP:

. . .

**Variables:** "$\pE y^\alpha$" for every multi-index $\alpha$ with $|\alpha| \leq t$ (assume $t$ even)

. . .

**They define an operator:** $\pE p(y) = \pE \sum p_\alpha y^\alpha = \sum p_\alpha \pE y^\alpha$

. . .

**Constraints (1) and (3):** $\pE 1 = \pE y^{\emptyset} = 1$ is a linear constraint. So is $\pE p_i(y) \cdot y^\alpha = 0$.

. . .

**Constraint (2):** $\pE p^2 \geq 0$ is equivalent to $p^\top M p \geq 0$ where $M_{\alpha,\beta} = \pE y^\alpha y^\beta$.

. . .

[Resulting SDP has "intended solution" $(y^{\tensor t/2})(y^{\tensor t/2})^\top$]{.magenta}\
[(compare with $yy^\top$ from basic SDP)]{.blue}

---

Final comments:

Run through the whole construction for robust mean estimation and will get an SDP with "intended solution"

$w^{\tensor t} \tensor \mu^{\tensor t} \tensor X^{\tensor t} \tensor g^{\tensor t}$

where $w$ is $0/1$ indicator of a set of $(1-\e)m$ samples with mean $\mu$, bounded covariance, and $X,g$ are auxiliary variables.





---

### Mixture Models

![](mixtures-small.png)


---

### Mixture Models

**Input:** Samples $X_1,\ldots,X_n \in \R^d$ from mixture of $\cD_1,\ldots,\cD_k$ with means $\mu_1,\ldots,\mu_k \in \R^d$

**Goal:** cluster $X_1,\ldots,X_n$ and/or estimate $\mu_1,\ldots,\mu_k$\

. . .

[**1890s:** Pearson *invents method of moments* to learn mixture of $k=2$ Gaussians in $d=1$ dimension]{.blue}

[**Now:** ubiquitous generative model of inhomogeneous data -- data from multiple populations]{.blue}

. . .

[Today, inhomogeneous data is high-dimensional and can have many underlying components]{.magenta}

[**Aim to use $\poly(d,k)$ samples and time**]{.magenta}


---


### Information-Theoretic Barrier

Mixture of $k$ Gaussians in $d=1$ dimension can be $2^{-\Omega(k)}$-close to standard Gaussian [[Moitra-Valiant]]{.gray}

![](info-issue.png)

---

### Separation Assumption

**Input:** Samples $X_1,\ldots,X_n \in \R^d$ from mixture of $\cD_1,\ldots,\cD_k$ with means $\mu_1,\ldots,\mu_k \in \R^d$

**Scaling:** Assume covariances $\Sigma_1,\ldots,\Sigma_k \preceq I$

**$\Delta$-Separation assumption:** $\|\mu_i - \mu_j\| \geq \Delta$ for some $\Delta > 0$.

. . .

![](delta-separated-small.png)

. . .

[**For which $\Delta > 0$ and which $\cD_1,\ldots,\cD_k$ can $\mu_1,\ldots,\mu_k$ be estimated in $\poly(d,k)$ time, samples?**]{.magenta .center}



---

### Mixture Models -- Non-SoS and SoS Results

For now: $\cD_i$ Gaussian, for simplicity, covariances $\Sigma \preceq I$, uniform mixture.

$\Delta$                      Algorithm                                 Property of Gaussians                  Reference
--------                      ---------                                 ---------------------                  ---------
$10\sqrt{d}$                  greedy                                    distance to mean                       [folklore]
$0.01\sqrt{d}$                spectral                                  bdd covariance                         [D99]
$d^{1/4}$                     EM (captured by greedy)                   pairwise distances                     [DS01]
$\min(d,k)^{1/4}$             PCA+EM/greedy                             pairwise distances                     [VW02]
[**$k^{\e}$**]{.blue}         [**sum of squares**]{.blue}               bdd $1/\e$ moments                     [HL18,KSS18,DKS18]

[**lower bound:** if $\Delta \leq o(\sqrt{ \log k})$, need $\gg \poly(d,k)$ samples]{.red} [RV17]


---

>**Theorem 1:** If $\Delta = k^\e$, can recover $\mu_i$'s and cluster up to $1/\poly(k)$ error in time, samples $d^{O(1)}k^{O(1/\e)}$.

>**Theorem 2:** If $\Delta = C\sqrt{ \log k}$, can recover $\mu_i$'s and cluster up to $1/\poly(k)$ error in time, samples $d^{O(1)}k^{O(\log k)}$, for a universal constant $C$.


---

### Proofs to Algorithms

Recall from yesterday:

[Simple identifiability proof $\rightarrow$ SoS identifiability proof $\rightarrow$ SDP-based algorithm]{.magenta}

. . .

[**Whiteboard time!**]{.blue .center}


---

### Certifiable Moment Boundedness

[**For which distributions $\cD$ can SoS robustly estimate the mean?**]{.magenta}\
[**For which $\Delta$-separated $\cD_1,\ldots\cD_k$ can SoS cluster and learn means?**]{.magenta}

Various names in literature: *certifiable subgaussianity, explicit boundedness*

. . .

In identifiability proofs, needed $\E_{X \sim \cD} \iprod{X - \mu, u}^t \leq t^{t/2} \|u\|^t$ for all $u \in \R^d$.

(Implies no event $\mathcal{E}$ with probability $\e$ influences the mean by more than $\e^{1-1/t}$)

. . .

[To SoS-ify the identifiability proof, will need]{.blue}

[$C^t t^{t/2} \cdot \|u\|^t - \E_{X \sim \cD} \iprod{X - \mu, u}^t = \sum s_i^2$]{.blue}

. . .

True for $t$-wise products (next slide) and rotations thereof

Also true for Poincare distributions (an isoperimetry property) $\rightarrow$ strongly log-concave distributions [KSS18]

---

### Certifiable moment bounds for product distributions

Let $X$ on $\R^d$ be $t$-wise independent, assume $\E X = 0$ and $\E X_i^t \leq B$.

. . .

Assume coordinates $X_i$ are symmetric about $0$ (otherwise replace with $X - X'$ for independent draw $X'$)

. . .

Then $\E X^\alpha = 0$ for any odd $\alpha$ with $|\alpha| \leq t$. [E.g $\E X_1^2 X_{10}^5 = 0$]{.blue}

. . .

$\E \iprod{X,u}^t = \sum_{|\alpha| = t} u^\alpha \E X^\alpha = \sum_{|\alpha| = t, \alpha \text{ even}} u^\alpha \E X^\alpha$

. . .

Let $c_\alpha = B - \E X^\alpha \geq 0$.

Then $B \cdot \|u\|^t - \E \iprod{X,u}^t = \sum_{|\alpha| = t, \alpha \text{ even}} c_\alpha u^\alpha$ is an SoS.


---

### Which distributions have certifiably bounded moments?

Known: Poincare (with dimension-independent constant), hence strongly log-concave

The frontier: log-concave (implied by KLS conjecture via Poincare?)

Moonshot: subgaussian?? (probably too broad)

[Open problem: prove a hardness result for some subgaussian distribution]{.blue}


---

### Wrapping up

If you remember only one thing: **simple identifiability proofs $\rightarrow$ computationally efficient algorithms.**

SoS offers provable guarantees for broadest known classes of distributions for clustering, robust moment estimation.

. . .

And robust regression, robust sparse recovery, $\ldots$

. . .

**Proofs to algorithms recipe** also works for dictionary learning, matrix/tensor completion, tensor principal component analysis, and more


. . .

[**Thanks!!**]{.magenta}


