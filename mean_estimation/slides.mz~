---
title: Sub-Gaussian Mean Estimation in Polynomial Time
tags: presentation
output:
  includes:
    in_header: header.html
---

(loading loading loading -- advance slide)

$\newcommand{\R}{\mathbb{R}}$
$\newcommand{\e}{\varepsilon}$
$\newcommand{\cD}{\mathcal{D}}$
$\newcommand{\poly}{\text{poly}}$
$\newcommand{\cN}{\mathcal{N}}$
$\newcommand{\tensor}{\otimes}$
$\newcommand{\E}{\mathop{\mathbb{E}}}$
$\renewcommand{\hat}{\widehat}$
$\newcommand{\iprod}[1]{\langle #1 \rangle}$
$\newcommand{\pE}{\tilde{\mathbb{E}}}$
$\newcommand{\Paren}[1]{\left ( #1 \right )}$
$\newcommand{\N}{\mathbb{N}}$
$\newcommand{\Tr}{\text{Tr}}$

---


# Mean Estimation with Sub-Gaussian Rates in Polynomial Time

[**Sam Hopkins (Miller Fellow, UC Berkeley)**]{.center .blue}



---

## estimating the mean of a random vector

. . .

>**input:** $X_1,\ldots,X_n \in \R^d$ independent copies of $X$\
>**output:** estimator $\hat{\mu}(X_1,\ldots,X_n)$ of $\mu = \E X$.
>

for this talk assume **isotropic:** [$\E (X - \mu)(X - \mu)^\top = I$]{.blue}

. . .

<hr>

[**obvious "algorithm" -- the empirical mean**]{.center}

*empirical mean:* [$\overline{\mu} = \tfrac 1 n \sum X_i$]{.blue}

*mean square error:* $\E \| \overline{\mu} - \mu \|^2 = \frac 1 n \E \|X - \mu\|^2 = \frac d n$

. . .

<hr>

[**what about the tail?**]{.center .magenta}

---

## tail of the empirical mean

first, $d=1$. [how small is $\Pr(| \overline{\mu} - \mu | > t)$?]{.magenta}

. . .

*confidence interval (c.i.):* $\Pr(| \overline{\mu} - \mu | > t) \leq \delta$ implies $\delta$-c.i. of width $t$

. . .

<hr>

**strong assumption:** $X$ is $\cN(\mu,1)$

**yields gaussian tail:** $\Pr( |\overline{\mu} - \mu| > t ) \leq \exp(-t^2 n)$

. . .

<hr>

**weak assumption:** $X$ has $\E (X - \mu)^2 \leq 1$

**apply Chebyshev:** $\Pr (| \overline{\mu} - \mu| > t) \leq \frac 1 {t^2 n}$

. . .

![](confidence.PNG)

---

## heavy tails

only assume $\E X, \E X^2$ are finite.


<div class="scratch-preview">
<iframe allowtransparenncy="true" width="800" height="400" src="gaussian_pdf.html" frameborder="0"></iframe>
</div>

also occur in high dimensions: corruptions, power laws in large networks, etc.

---

## beyond the empirical mean

[is there an estimator $\hat{\mu}$ with $\Pr( |\overline{\mu} - \mu| > t ) \leq \exp(-\Omega(t^2 n))$ when $X$ is heavy-tailed?]

. . .

no. [**but you can fake it!**]{.fragment .red}

. . .

for every $t$, exists $\hat{\mu}_t$ such that $\Pr( | \hat{\mu}_t - \mu | > t ) \leq \exp(-\Omega(t^2 n))$

---



## Confidence Intervals

![](confidence_interval.svg)


---


## Confidence Intervals / Non-Asymptotics: Gaussians

If $X_1,\ldots,X_n \sim \cN(\mu, \Sigma)$ then [$\overline{\mu} = \frac 1 n \sum_{i=1}^n X_i \sim \cN(\mu, \Sigma / n)$]{.blue}

. . .

$\delta$ confidence interval radius: [$\sqrt{\frac{\Tr \Sigma}{n}} + \sqrt{\frac{2 \|\Sigma\| \log(1/\delta)}{n}}$]{.blue}

. . .

\

[**Consider $\Sigma = I, \mu = 0$**]{.center}

. . .

$\overline{\mu} \sim \cN(0, I/n)$, so $\|\overline{\mu}\|^2 = \sum_{i=1}^d g_i^2 \approx \frac{d \pm \sqrt{d \log (1/\delta)}}{n}$ [**norm of std. Gaus.**]{.red}

\

. . .

[$\|\overline{\mu}\| = \sqrt{\sum_{i=1}^d g_i^2} \approx \sqrt{\frac d n } \cdot \Paren{1 \pm \frac {\sqrt{\log 1/\delta}} {\sqrt d}} = \sqrt{\frac dn} + \sqrt{\frac{1 \cdot \log(1/\delta)}{n}}$]{.magenta}


---


## Heavy Tails

Only assume $\mu = \E X$ and $\Sigma = \E(X - \mu)(X - \mu)^\top$ are finite.


<div class="scratch-preview">
<iframe allowtransparenncy="true" width="800" height="400" src="gaussian_pdf.html" frameborder="0"></iframe>
</div>

Heavy tails do occur in high dimensions: corruptions, power laws in large networks, etc.


---


## Heavy Tails and the Empirical Mean

[$\Tr \Sigma = \E \|X - \mu\|^2$ and $\|\Sigma\|=$ magnitude of principal component.]{.center}

. . .


[**Sub-Gaussian $X$**]{.center}

$$r_\delta = \sqrt{\frac{\Tr \Sigma}{n}} + \sqrt{\frac{2 \|\Sigma\| \log(1/\delta)}{n}}$$

. . .

[[$1/\sqrt{n}$ rate]{.cyan}, [$\sqrt{\log 1/\delta}$ tail bound,]{.fragment .blue} [$\sqrt{\log 1/\delta}$ multiplies $\sqrt{\|\Sigma\|}$]{.fragment .magenta}]{.center}

. . .

[**Heavy-tailed $X$**]{.center}

$$ r_\delta = \sqrt{\frac{\Tr \Sigma}{n \delta}} $$

. . .

[$1/\sqrt{n}$ rate]{.cyan}\
[$\sqrt{1/\delta}$ tail bound [**crisis of confidence**]{.right}]{.fragment .blue}\
[$\sqrt{1/\delta}$ multiplies $\sqrt{\Tr \Sigma}$ [**curse of dimensionality**]{.right}]{.fragment .magenta}


---

For which r.v.'s $X$ can $\E X$ be estimated with [**sub-Gaussian-like confidence intervals**]{.blue}?


. . .


[[**Only need bounded 2nd moments!**]{.red} [[Lugosi-Mendelson 18]]{.grey} ]{.center}

. . .

[But if $X \in \R^d$, it takes $\exp(d)$ time to compute]{.center}

. . .

\
\

For which r.v.'s $X$ is there a [**polynomial-time computable**]{.magenta} and [**sub-Gaussian**]{.blue} estimator?


. . .

[**This work: only need bounded 2nd moments!**]{.center .red}

---

## Main Theorem

>**Theorem:**  Given $X_1,\ldots,X_n,\delta$, can find $\hat{\mu}$ such that
>
>$$
\Pr \left \{ \left \| \hat{\mu} - \mu \right \| > C \left ( \sqrt{\frac{\Tr \Sigma}{n}} + \sqrt{\frac{\|\Sigma\| \log(1/\delta)}{n}} \right ) \right \} \leq \delta \, .
$$
>
>in time $O(nd) + (d \log(1/\delta))^{O(1)}$.


. . .

\
\

**Main innovation:** new semidefinite programming (SDP) algorithm for high-dimensional median, based on sum of squares method (SoS).

[**SoS familiarity is not a prerequisite for this talk.**]{.magenta}



---

## Prior Work

covariance $\Sigma$, $n$ samples, confidence $1-\delta$

. . .

[$\text{good} = O \Paren{\sqrt{\frac{\Tr \Sigma}{n}} + \sqrt{\frac{\|\Sigma\| \log(1/\delta)}{n}}}$]{.blue}

. . .

[$\text{curse} = O \Paren{\sqrt{\frac{\Tr \Sigma \log(1/\delta) }{n}}}$ [sub-Gaussian, dimension **dependent** tail]{.right}]{.magenta}

. . .

[$\text{both} = O \Paren{\sqrt{\frac{\Tr \Sigma}{\delta n}}}$ [**heavy**, dimension **dependent** tail]{.right}]{.red}

. . .


Estimator                Dimension                $r_\delta$               Time                  Reference
---------                ---------                ----------               ----                  ---------
empirical mean           any                      [both]{.red}              poly                  folklore
(many)                   $1$                      [good]{.blue}            poly                  (many)
geometric median         any                      [curse]{.magenta}         poly                  [[Minsker 13, Hsu-Sabato 13]]{.small}
tournament median        any                      [good]{.blue}            exp                   [[Lugosi-Mendelson 18]]{.small}
**median-SDP**           **any**                  [**good**]{.blue}        **poly**              **this work**

---

## Agenda

1. The $d=1$ case: median of means
2. The Lugosi-Mendelson estimator
3. median-SDP

---

## Median of Means: $d=1$

[[Nemirovsky-Yudin, Jerrum-Valiant-Vazirani, Alon-Matias-Szegedy]]{.cite}


\

$X_1,\ldots,X_n$ i.i.d. copies of $X$ with $\E X = \mu, \E (X - \mu)^2 = \sigma^2$.

\

. . .

[Empirical mean: $r_\delta = O \Paren{ \sqrt{\frac{\sigma^2}{n\delta}}}$.]{.blue}

\

. . .

[Median of means: $r_\delta = O \Paren{ \sqrt{\frac{\sigma^2 \log(1/\delta)}{n}}}$.]{.magenta}

\

[**$1/\delta$ becomes $\log(1/\delta)$**]{.cyan} [no more crisis of confidence]{.red}

---

![](diagram.PNG)


---

1. [oblivously assign samples to $k$ buckets of size $n/k$, with $k = \Theta(\log(1/\delta))$.]{.fragment}
2. [reduce variance -- [let $Z_i = \tfrac kn \sum_{i \text{ in $j$-th bucket}} X_i$]{.cyan}]{.fragment}
3. [binomial tail for free -- [let $\hat{\mu} =$ any median of $Z_1,\ldots,Z_k$]{.cyan}]{.fragment}

[![](median-of-means-1.svg)]{.fragment}\
[![](median-of-means-2.svg)]{.fragment}\
[![](median-of-means-3.svg)]{.fragment}

---

\
\
\
\
\

**Key insight: *number of outliers* concentrates even when *sum of outliers* does not.**

---

### Analysis: $d=1$

$Z_1,\ldots,Z_k$ are bucketed averages, $\E Z = \mu$ and $\E(Z - \mu)^2 = \tfrac kn \cdot \sigma^2$

[At least $2k/3$ $Z_i$'s have $|Z_i - \mu| < r$ $\rightarrow$ $|\text{median}(Z_1,\ldots,Z_k) - \mu| \leq r$.]{.blue}

. . .

\
\

![](medgood.PNG)

---

### Analysis: $d=1$

$Z_1,\ldots,Z_k$ are bucketed averages, $\E Z = \mu$ and $\E(Z - \mu)^2 = \tfrac kn \cdot \sigma^2$

[At least $2k/3$ $Z_i$'s have $|Z_i - \mu| < r$ $\rightarrow$ $|\text{median}(Z_1,\ldots,Z_k) - \mu| \leq r$.]{.blue}



[**Chebyshev on each bucket:**
$$\Pr(|Z_i - \mu| > r) \leq \frac{\text{Var}(Z_i)}{r^2} = \frac{\tfrac kn \cdot \sigma^2}{r^2}$$]{.magenta}


**Buckets form binomial distribution:** $\Pr(\text{more than } k/3 \text{ $Z_i$'s have } |Z_i - \mu| \gg \sqrt{k\sigma^2/n}) \leq \exp(-\Omega(k)) = \delta$.

![](binom_tail.svg)

. . .

[So $| \text{median}(Z_1,\ldots,Z_k) - \mu | \leq O\Paren{\sqrt{ k \sigma^2/n}} = O\Paren{\sqrt{ \sigma^2 \log(1/\delta)/n}}$]{.cyan}

---

### Median of means in higher dimensions

**Goal:** estimator with $r_\delta = O \Paren{\sqrt{\Tr \Sigma / n} + \sqrt{\|\Sigma\| \log(1/\delta)/n}}$

. . .

**Buckets:** $Z_1,\ldots,Z_k$ with $\E Z = \mu$ and $\E (Z - \mu)(Z-\mu)^\top = \Gamma = \tfrac kn \Sigma$.

\

. . .

[**What happens with probability $1-e^{-k/10}$ ?**]{.magenta .center}

\

. . .

**$d=1$:** [at least $\tfrac 23 k$ $Z_i$'s have $|Z_i - \mu| \leq 10 \sqrt{\text{Var}(Z)}$]{.blue}

\

. . .

**$d > 1$:**  [at least $\tfrac 23 k$ $Z_i$'s have]{.blue} [$\|Z_i - \mu \| \leq 10 \sqrt{\Tr \Gamma} = 10 \sqrt{\E \|Z - \mu\|^2}$]{.red}

. . .

\

[Result: **dimension-dependent** rate $r_\delta = O\Paren{\sqrt{\frac{\Tr \Sigma \log(1/\delta)}{n}}}$.]{.cyan}





---

## Lugosi and Mendelson's median

. . .

[Cannot ask for $2k/3$ $Z_i$'s to be non-outliers]{.magenta}

. . .

[Instead, ask for every direction to have at most $k/3$ outliers!]{.magenta}

---


![](outlier-directions-1.svg)


---

![](outlier-directions-2.svg)


---

![](outlier-directions-3.svg)

---



## Lugosi and Mendelson's estimator


**Input:** $X_1,\ldots,X_n,\delta$

. . .

**Buckets:** bucketed means $Z_1,\ldots,Z_k$ for $k = \log(1/\delta)$

. . .

**Output $\hat{\mu}$:** any $x$ such that in all directions $u$, for at least $2k/3$ $Z_i$

$$\iprod{Z_i,u}  - \iprod{x,u} \leq r$$

. . .


[**Alternative interpretation:** if max value is $r$ then $\hat{\mu}$ has distance at most $r$ to being a median in every direction.]{.cyan}


---


![](shift-to-median-1.svg)

---

![](shift-to-median-2.svg)


---


## Main lemma for Lugosi-Mendelson estimator

Remember: $X_1,\ldots,X_n$ samples in $k = \Theta( \log(1/\delta))$ buckets, $Z_i$ is mean in $i$-th bucket.

\

[**Lemma:** If $r \gg \sqrt{\Tr \Sigma / n} + \sqrt{\|\Sigma\| \log(1/\delta) / n}$ then w.p. $1-\delta$,]{.blue}

[for every unit vector $u \in \R^d$]{.fragment .blue}

[exist at most $k/3$ $Z_i$'s s.t. $\iprod{Z_i, u} > \iprod{\mu, u} + r$.]{.fragment .blue}

[Furthermore, any two points $x,y$ having this property also satisfy $\|x - y\| \leq 2r$]{.fragment .magenta}

\
\

. . .

**But no obvious algorithm!**



---

## Median SDP

(Almost) a convex (SDP) relaxation of:

[$$
\{ x \, : \, \text{ for all $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{x,u} + r$} \}
$$]{.blue}

. . .

with enough constraints that [each step of the Lugosi-Mendelson analysis also applies to the SDP]{.magenta} (the heart of the "proofs to algorithms" SoS method)

. . .

**$\text{poly}(d,k)$ constraints enough to capture all important properties of integral solutions.**

. . .

---

[$$
\{ x \, : \, \text{ for all $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{x,u} + r$} \}
$$]{.blue}

[How would you know if you found such $x$?]{.blue .center}

\

[**Lemma 1:** If $r \gg \sqrt{\Tr \Sigma / n} + \sqrt{\|\Sigma\| \log(1/\delta) / n}$ then w.p. $1-\delta$,]{.fragment}

[for every unit vector $u \in \R^d$]{.fragment}

[exist at most $k/3$ $Z_i$'s s.t. $\iprod{Z_i, u} > \iprod{\mu, u} + r$.]{.fragment}

\

[**Lemma 2:** Furthermore, any two points $x,y$ having this property also satisfy $\|x - y\| \leq 2r$]{.fragment}


---

[$$
\{ x \, : \, \text{ for all $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{x,u} + r$} \}
$$]{.blue}

[How would you know if you found such $x$?]{.blue .center}

\

[**Algorithmic Lemma 1:** If $r \gg \sqrt{\Tr \Sigma / n} + \sqrt{\|\Sigma\| \log(1/\delta) / n}$ w.p. $1-\delta$ there is a *certificate* $M_\mu$]{.fragment}

[which *proves* that for every unit vector $u \in \R^d$]{.fragment}

[exist at most $k/3$ $Z_i$'s s.t. $\iprod{Z_i, u} > \iprod{\mu, u} + r$.]{.fragment}

\

[**Algorithmic Lemma 2:** there is a polynomial-time algorithm which finds $x$ such that $\|x - y\| \leq 2r$ if $y$ has a certificate $M_y$.]{.fragment}

. . .

\

[$$
\{ (x,M_x) \, : \, \text{ $M_x$ proves for all $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{x,u} + r$} \}
$$]{.blue}


---

## Certifying the median

**Change of notation:** $Z$ has mean $\mu$ and covariance $\Sigma$ ($X$ has disappeared)

>**Problem:** given $Z_1,\ldots,Z_k,r$ [*and* $x$]{.magenta}, [certify]{.blue} that for all unit $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{\mu,u} + r$.
>
>**Goal:** certification succeeds w.p. at least $1 - \exp(-k/10)$ when $r = C\Paren{\sqrt{\Tr \Sigma / k} + \sqrt{\|\Sigma\|}}$ [*and $x = \mu$*]{.magenta}


(If $Z_1,\ldots,Z_k$ are bucketed means of $X_1,\ldots,X_n$ and $k = C \log(1/\delta)$, recover previous setting.)

. . .

[**What is certification?**]{.blue}

. . .

[Algorithm takes $Z_1,\ldots,Z_k,r,x$, outputs "yes" or "I don't know"]{.blue}

. . .

[output is yes $\rightarrow$ for all unit $u$, at most $k/3$ $Z_i$'s have $\iprod{Z_i,u} > \iprod{x,u} + r$]{.blue}




---

![](outlier-directions-4.svg)

---

### The Median Certification SDP

Start with a quadratic program in $b = b_1,\ldots,b_k, u = u_1,\ldots,u_d$:

$$\max_{u,b} \sum b_i \text{ such that } b_i^2 = b_i, \|u\|^2 = 1, \text{ and } b_i \iprod{Z_i,u} \geq b_i (\iprod{\mu,u} + r)$$

. . .

\

[Relax $(b,u)(b,u)^\top$ to block PSD matrix]{.magenta .center}

[$$\left ( \begin{array}{cc} B & W \\ W^\top & U \end{array} \right ) $$]{.magenta}

. . .

\



[
\begin{align}
SDP(Z_1,\ldots,Z_k,\mu)  = & \max \, \text{Tr} B \text{ such that} \\
& B_{ii} \leq 1 \\
& \Tr U = 1 \\
& \iprod{Z_i, W_i} \geq \iprod{\mu, W_i} + r B_{ii}
\end{align}
]{.blue .center}

---

## Glimpse of the analysis

**Goal:** if $r \gg \Paren{\sqrt{\Tr \Sigma / k} + \sqrt{\|\Sigma\|}}$ then w.p. $1-e^{-k/10}$,

$$SDP(Z_1,\ldots Z_k, \mu) \leq k/3$$

[Then **dual solution** is a (degree $2$ SoS) proof $M_\mu$]{.fragment .red}

\

. . .

[**Expectation:** $\E SDP(Z_1,\ldots,Z_k,\mu) \leq k/6$]{.blue}

[*proof step 1:* relate $SDP(Z_1,\ldots,Z_k,\mu)$ to $2 \rightarrow 1$ norm of a random matrix]{.blue}

. . .

[*proof step 2:* Grothendieck's inequality for SDP approximation of $2 \rightarrow 1$ norm]{.blue}

\

. . .

[**Concentration:** $\Pr ( SDP(Z_1,\ldots,Z_k,\mu) - \E SDP(Z_1,\ldots,Z_k,\mu) > k/6 ) < e^{-k/10}$]{.magenta}

[*proof sketch:* $SDP(Z_1,\ldots,Z_k,\mu)$ satisfies a **bounded differences** property -- pays at most $1$ per outlier.]{.magenta}


---

## Proof insights

[**SDPs and robust matrix norms:**]{.center}

usual norms of $Z_1,\ldots,Z_k$ are bad, e.g. $\left \| \sum_{i =1}^k Z_i Z_i^\top \right \| = \|(Z_1,\ldots,Z_k)^\top\|_{2 \rightarrow 2}^2$

SDP can handle less outlier-sensitive norms, e.g. $\| (Z_1,\ldots,Z_k)^\top \|_{2 \rightarrow 1}$

. . .

\
\

[**SDPs and stability:**]{.center}

unlike norms and averages, SDP can move by $ \leq 1$ even if $Z_i$ moves by $10^{10}$.

SDPs can concentrate better than norms and averages

---

**Algorithmic Lemma 1:** If $r \gg \sqrt{\Tr \Sigma / n} + \sqrt{\|\Sigma\| \log(1/\delta) / n}$ w.p. $1-\delta$ there is a *certificate* $M_\mu$

which *proves* that for every unit vector $u \in \R^d$

exist at most $k/3$ $Z_i$'s s.t. $\iprod{Z_i, u} > \iprod{\mu, u} + r$.

\

[**Algorithmic Lemma 2:** there is a polynomial-time algorithm which finds $x$ such that $\|x - y\| \leq 2r$ if $y$ has a certificate $M_y$.]{.fragment}

\

[Prove in degree 8 SoS that $(x,M_x), (y,M_y)$ must have $\|x-y\| \leq 2r$, get an algorithm for free.]{.blue .fragment}


---

## Conclusion

**Main theorem:** first polynomial-time estimator for heavy-tailed estimation [matching empirical mean's performance in Gaussian setting]{.magenta}


. . .

**Open problem:** is there a *practical algorithm* whose *empirical* performance improves on state-of-the-art for heavy-tailed estimation?


. . .

\

[**Thanks!**]{.center}



