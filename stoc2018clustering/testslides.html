<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Clustering and Robust Moment Estimation via SoS Proofs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Clustering and Robust Moment Estimation via SoS Proofs</h1>
</section>

<section class="slide level1">

<p>(loading loading loading – advance slide)</p>
<p><span class="math inline">\(\newcommand{\P}{\mathbb{P}}\)</span> <span class="math inline">\(\newcommand{\R}{\mathbb{R}}\)</span> <span class="math inline">\(\newcommand{\e}{\varepsilon}\)</span> <span class="math inline">\(\newcommand{\cD}{\mathcal{D}}\)</span> <span class="math inline">\(\newcommand{\poly}{\text{poly}}\)</span> <span class="math inline">\(\newcommand{\cN}{\mathcal{N}}\)</span> <span class="math inline">\(\newcommand{\tensor}{\otimes}\)</span> <span class="math inline">\(\newcommand{\E}{\mathop{\mathbb{E}}}\)</span> <span class="math inline">\(\renewcommand{\hat}{\widehat}\)</span></p>
</section>
<section id="clustering-and-robust-moment-estimation-via-sos-proofs" class="slide level1">
<h1>Clustering and Robust Moment Estimation via SoS Proofs</h1>
<p><span class="blue"><em>Mixture Models, Robustness, and Sum of Squares Proofs</em></span><br />
Sam Hopkins (Cornell University)<br />
Jerry Li (MIT)</p>
<p><span class="blue"><em>Robust Moment Estimation and Improved Clustering via Sum of Squares</em></span> Pravesh Kothari (Princeton University/IAS)<br />
Jacob Steinhardt (Stanford)<br />
David Steurer (ETH Zurich)</p>
<p><em>(This talk presents the union of results from two papers.)</em></p>
</section>
<section id="moment-estimation" class="slide level1">
<h1>Moment Estimation</h1>
<p>Unknown distribution <span class="math inline">\(\cD\)</span> on <span class="math inline">\(\R^d\)</span> with some bounded moments</p>
<p><strong>Goal:</strong> given samples <span class="math inline">\(x_1,\ldots,x_n \sim \cD\)</span>, estimate</p>
<p><span class="math display">\[\begin{align*}
&amp; \text{mean: } \mu = \E_{x \sim \cD} x\\
&amp; \text{higher moments: } \Sigma = \E_{x \sim \cD} xx^\top, \E_{x \sim \cD} x^{\tensor 4}, \ldots
\end{align*}\]</span></p>
<div class="fragment">
<p><strong>Easy: empirical mean, empirical covariance, etc.</strong></p>
<p><span class="math display">\[
\frac 1n \sum_{i \leq n} x_i, \quad \frac 1n \sum_{i \leq n} x_i x_i^\top, \ldots
\]</span></p>
</div>
<div class="fragment">
<p>Require <span class="math inline">\(\poly(d)\)</span>-many samples, computable in <span class="math inline">\(\poly(d)\)</span> time, excellent convergence guarantees</p>
</div>
</section>
<section class="slide level1">

<h2 id="what-if-only-a-subset-of-data-from-cd">What if Only a Subset of Data from <span class="math inline">\(\cD\)</span>?</h2>
<p>Unknown distribution <span class="math inline">\(\cD\)</span> on <span class="math inline">\(\R^d\)</span> with some bounded moments</p>
<p>Still get <span class="math inline">\(x_1,\ldots,x_n \in \R^d\)</span>, but <strong>only an <span class="blue">unknown</span> <span class="math inline">\(\alpha\)</span>-fraction are from</strong> <span class="math inline">\(\cD\)</span>.<br />
</p>
<p><span class="magenta center"><strong>Can you still estimate moments of <span class="math inline">\(\cD\)</span>?</strong></span></p>
</section>
<section class="slide level1">

<h2 id="mixture-models">Mixture Models</h2>
<p><span class="math inline">\(\alpha = 1/k\)</span>, mixture of <span class="math inline">\(k\)</span> distributions <span class="math inline">\(\cD_1,\ldots,\cD_k\)</span></p>
<p>Standard generative model for <em>clustering</em> and <em>heterogeneous data</em> <img data-src="mixtures-small.png" /></p>
</section>
<section class="slide level1">

<h2 id="robust-statistics">Robust Statistics</h2>
<p><span class="math inline">\(\alpha = 1-\e\)</span>, remaining <span class="math inline">\(\e\)</span> fraction <em>chosen adversarially</em></p>
<p>Models <em>data poisoning</em>, <em>model misspecification</em>, and more</p>
<p><img data-src="eps-corrupted-small.png" /></p>
<p><span class="center"><span class="blue">blue = clean data</span>, <span class="red">red = outliers</span></span></p>
</section>
<section class="slide level1">

<h3 id="contributions">Contributions</h3>
<p>New polynomial-time technique for <span class="blue">estimating mean and higer moments</span> when <span class="blue">only a subset</span> of data comes from <span class="math inline">\(\cD\)</span>, under <span class="blue">mild conditions on <span class="math inline">\(\cD\)</span></span>.</p>
<hr>
<div class="fragment">
<p><span class="center"><strong>In this talk</strong></span></p>
<p><strong>New algorithm for learning <span class="blue">mixtures</span> of well-separated distributions</strong><br />
</p>
<p><strong>New algorithm for mean and higher-moment estimation from <span class="blue"><span class="math inline">\(\e\)</span>-corrupted samples</span></strong> – application: automatic robustification of moment-method algorithms</p>
<hr>
</div>
<div class="fragment">
<p><span class="center"><strong>Cut for time</strong></span></p>
<p>New algorithm for <span class="blue">list learning</span>, a common generalization of mixture models and <span class="math inline">\(\e\)</span>-corrupted estimation [BBV08,CSV17]</p>
</div>
<div class="fragment">
<blockquote>
<p>Our algorithms are <span class="magenta">distribution-agnostic</span> and work for <em>any distribution with <strong>certifiably</strong> bounded moments</em>: <span class="blue">log-concave, affine transforms of products,…</span></p>
</blockquote>
</div>
</section>
<section id="agenda" class="slide level1">
<h1>Agenda</h1>
<ol type="1">
<li>Mixture models – history and results</li>
<li>Robust moment estimation – history and results</li>
<li>Algorithm and analysis for mixture models</li>
</ol>
</section>
<section class="slide level1">

<h3 id="mixture-models-1">Mixture Models</h3>
<p><img data-src="mixtures-small.png" /></p>
</section>
<section class="slide level1">

<h3 id="mixture-models-2">Mixture Models</h3>
<p>1890s: Pearson <em>invents method of moments</em> to learn mixture of <span class="math inline">\(k=2\)</span> Gaussians in <span class="math inline">\(d=1\)</span> dimension.</p>
<div class="fragment">
<p><strong>Our setting:</strong> <span class="math inline">\(d,k\)</span> large, want alg using <span class="math inline">\(\poly(d,k)\)</span> samples/time</p>
<p><strong><span class="math inline">\(\Delta\)</span>-Separation assumption:</strong> <span class="math inline">\(\|\mu_i - \mu_j\| \geq \Delta\)</span> for some <span class="math inline">\(\Delta &gt; 0\)</span>.</p>
<p><span class="magenta center"><strong>For which <span class="math inline">\(\Delta &gt; 0\)</span> is this possible?</strong></span></p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img data-src="delta-separated.png" /></p>
</div><div class="column" style="width:&quot;45%;">
<blockquote>
<p><span class="fragment small">One way around [Moitra-Valient] obstacle: <span class="math inline">\(k\)</span> Gaussians can require <span class="math inline">\(2^{-\Omega(k)}\)</span> samples</span> <span class="fragment"><img data-src="info-issue.png" /></span></p>
</blockquote>
</div>
</div>
</div>
</section>
<section class="slide level1">

<h3 id="mixture-models-previous-results-and-our-contribution">Mixture Models – Previous Results and Our Contribution</h3>
<p>Rest of talk: <span class="math inline">\(\cD_i\)</span> Gaussian, for simplicity, covariances <span class="math inline">\(\Sigma \preceq I\)</span>, uniform mixture.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(\Delta\)</span></th>
<th>Algorithm</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(10\sqrt{d}\)</span></td>
<td>greedy</td>
<td>[folklore]</td>
</tr>
<tr class="even">
<td><span class="math inline">\(0.01\sqrt{d}\)</span></td>
<td>spectral</td>
<td>[D99]</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(d^{1/4}\)</span></td>
<td>EM (captured by greedy)</td>
<td>[DS’01]</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\min(d,k)^{1/4}\)</span></td>
<td>PCA+EM/greedy</td>
<td>[VW02]</td>
</tr>
<tr class="odd">
<td><span class="red"><span class="math inline">\(o(\sqrt{ \log k})\)</span></span></td>
<td><span class="red">(need <span class="math inline">\(\gg \poly(d,k)\)</span> samples)</span></td>
<td>[RV17]</td>
</tr>
<tr class="even">
<td><span class="blue"><strong><span class="math inline">\(k^{\e}\)</span></strong></span></td>
<td><span class="blue"><strong>sum of squares</strong></span></td>
<td>[our works]</td>
</tr>
</tbody>
</table>
<div class="fragment">
<p><em>Similar result for spherical Gaussians by [DKS18], next talk</em></p>
<blockquote>
<p><strong>Theorem 1:</strong> For every <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math inline">\(\Delta = k^{\varepsilon}\)</span>, can recover <span class="math inline">\(\mu_i\)</span>’s (and cluster) up to <span class="math inline">\(1/\poly(k)\)</span> error in time <span class="math inline">\(d^{O(1)}k^{O(1/\varepsilon)}\)</span>.</p>
</blockquote>
</div>
</section>
<section class="slide level1">

<h3 id="mixture-models-previous-results-and-our-contribution-1">Mixture Models – Previous Results and Our Contribution</h3>
<p>Rest of talk: <span class="math inline">\(\cD_i\)</span> Gaussian, for simplicity, covariances <span class="math inline">\(\Sigma \preceq I\)</span>, uniform mixture.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(\Delta\)</span></th>
<th>Algorithm</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(10\sqrt{d}\)</span></td>
<td>greedy</td>
<td>[folklore]</td>
</tr>
<tr class="even">
<td><span class="math inline">\(0.01\sqrt{d}\)</span></td>
<td>spectral</td>
<td>[D99]</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(d^{1/4}\)</span></td>
<td>EM (captured by greedy)</td>
<td>[DS’01]</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\min(d,k)^{1/4}\)</span></td>
<td>PCA+EM/greedy</td>
<td>[VW02]</td>
</tr>
<tr class="odd">
<td><span class="red"><span class="math inline">\(o(\sqrt{ \log k})\)</span></span></td>
<td><span class="red">(need <span class="math inline">\(\gg \poly(d,k)\)</span> samples)</span></td>
<td>[RV17]</td>
</tr>
<tr class="even">
<td><span class="blue"><strong><span class="math inline">\(k^{\e}\)</span></strong></span></td>
<td><span class="blue"><strong>sum of squares</strong></span></td>
<td>[our works]</td>
</tr>
</tbody>
</table>
<p><em>Similar result for spherical Gaussians by [DKS18], next talk</em></p>
<blockquote>
<p><strong>Theorem 2:</strong> If <span class="math inline">\(\Delta = C\sqrt{ \log k}\)</span>, can recover <span class="math inline">\(\mu_i\)</span>’s (and cluster) up to <span class="math inline">\(1/\poly(k)\)</span> error in time <span class="math inline">\(d^{O(1)}k^{O(\log k)}\)</span>, for a universal constant <span class="math inline">\(C\)</span>.</p>
</blockquote>
</section>
<section class="slide level1">

<h3 id="robust-moment-estimation">Robust Moment Estimation</h3>
<p>Unknown <span class="math inline">\(\cD\)</span> on <span class="math inline">\(\R^d\)</span>, receive <span class="math inline">\(x_1,\ldots,x_n \in \R^d\)</span> which are <span class="red"><strong><span class="math inline">\(\e\)</span>-corrupted</strong></span>.</p>
<p>“Obvious” algorithms for estimating the <span class="blue">mean <span class="math inline">\(\mu\)</span></span> (outlier removal, etc) have error growing with <span class="math inline">\(d\)</span>. (e.g. <span class="blue"><span class="math inline">\(\|\hat{\mu} - \mu\| \leq O(\e \sqrt d)\)</span></span>)<br />
Ex. <span class="math inline">\(\cN(0,I)\)</span>:</p>
<p><span class="center"><img data-src="sqrt-d-small.png" alt="Gaussian example" /></span></p>
</section>
<section class="slide level1">

<p>Tukey, 1960: Dimension-independent error but not poly-time</p>
<p>Similar problems (learning halfspaces, PCA with adversarial corruptions), poly time [XCS10, ABL14]</p>
<p><strong>Recent breakthrough:</strong> first <em>poly-time, dimension-independent</em> guarantees for robust mean estimation, for distributions with bounded second moments [DKK+16, LRV16]. (e.g. <span class="blue"><span class="math inline">\(\|\hat{\mu} - \mu\| \leq O(\sqrt \e)\)</span></span>)</p>
<hr>
<div class="fragment">
<p><strong>Higher moments:</strong> Existing results limited to covariance estimation for Gaussian/<span class="math inline">\(4\)</span>-wise independent dist’ns [DKK+17, SCV18]</p>
<blockquote>
<p><strong>Kothari-Steinhardt-Steurer (this talk):</strong> first <em>poly-time</em> algorithms to estimate higher moments with <em>dimension-independent error</em>, non-Gaussian/<span class="math inline">\(k\)</span>-wise indep. distributions<br />
<span class="blue"><em>Automatic robustification of moment-method algorithms!</em></span></p>
</blockquote>
</div>
<div class="fragment">
<p><strong>Application:</strong> robust <span class="blue">independent component analysis</span>, via <span class="math inline">\(4\)</span>th moments</p>
</div>
</section>
<section class="slide level1">

<h2 id="opening-the-hoodback-to-mixture-models">Opening the hood…(back to mixture models)</h2>
</section>
<section class="slide level1">

<p><span class="blue">Main technique: <strong>algorithms from simple identifiability proofs</strong></span></p>
<p>Simplify setting: assume covariances <span class="math inline">\(\Sigma_i = I\)</span> (“spherical”)</p>
<blockquote>
<p><strong>Identifiability:</strong> For <span class="math inline">\(\Delta \geq k^\e\)</span> and typical <span class="math inline">\(x_1,\ldots,x_n \sim \sum \cN(\mu_i, I)\)</span>, if <span class="math inline">\(v_1,\ldots,v_k\)</span> such that <span class="math display">\[
\Pr_{\frac 1k \sum \cN(v_i,I)} (x_1,\ldots,x_n) \approx \Pr_{\frac 1k \sum \cN(\mu_i,I)} (x_1,\ldots,x_n)
\]</span> then <span class="math inline">\(\{v_1,\ldots,v_k\} \approx \{\mu_1,\ldots,\mu_k\}\)</span>.<br />
<span class="magenta">i.e. <span class="math inline">\(\mu_1,\ldots,\mu_k\)</span> are information-theoretically recoverable</span></p>
</blockquote>
<div class="fragment">
<p><span class="center"><strong>Sum of Squares method: turns a sufficiently-simple proof of identifiability into a polynomial-time semidefinite programming algorithm to find <span class="math inline">\(\mu_1,\ldots,\mu_k\)</span></strong></span></p>
<p>Now well-established, captures algorithms for matrix completion, sparse vector problems, dictionary learning, component analysis problems, tensor problems, and more</p>
</div>
</section>
<section class="slide level1">

<h2 id="algorithm-outline">Algorithm outline</h2>
<p>If <span class="math inline">\(\e &gt; 0\)</span>, separation is <span class="math inline">\(\Delta &gt; k^{\e}\)</span> and have <span class="math inline">\(n \geq k^{O(1)} d^{O(1/\e)}\)</span> samples.</p>
<p><strong>Input:</strong> <span class="math inline">\(x_1,\ldots,x_n \in \R^d\)</span></p>
<div class="fragment">
<ol type="1">
<li>Solve an <span class="math inline">\(n^{O(1/\e)}\)</span>-size semidefinite program (from Sum of Squares hierarchy) designed to find a subset <span class="math inline">\(S\)</span> of <span class="math inline">\(n/k\)</span> samples with <em>bounded <span class="math inline">\(O(1/\e)\)</span>-th empirical moments</em></li>
</ol>
<p><span class="math display">\[
\forall \|v\|=1, \, \frac 1 {|S|} \sum_{i \in S} \langle x_i- \mu(S), v \rangle^{10/\e} \leq O_\e(1)
\]</span></p>
</div>
</section>
<section class="slide level1">

<h2 id="algorithm-outline-1">Algorithm outline</h2>
<p>If <span class="math inline">\(\e &gt; 0\)</span>, separation is <span class="math inline">\(\Delta &gt; k^{\e}\)</span> and have <span class="math inline">\(n \geq k^{O(1)} d^{O(1/\e)}\)</span> samples.</p>
<p><strong>Input:</strong> <span class="math inline">\(x_1,\ldots,x_n \in \R^d\)</span></p>
<ol type="1">
<li><p>Solve an <span class="math inline">\(n^{O(1/\e)}\)</span>-size semidefinite program (from Sum of Squares hierarchy) designed to find a subset <span class="math inline">\(S\)</span> of <span class="math inline">\(n/k\)</span> samples with <em>bounded <span class="math inline">\(O(1/\e)\)</span>-th empirical moments</em></p></li>
<li><p>SDP solution is (nearly) integral (trivial rounding finds clusters and means)</p></li>
</ol>
<hr>
<div class="fragment">
<p><strong>Analysis outline:</strong></p>
<ol type="1">
<li><p>Dual of the SDP <span class="math inline">\(=\)</span> a restricted proof system (“the degree <span class="math inline">\(O(1/\e)\)</span> SoS proof system”), captures inequalities like <em>Cauchy-Schwarz</em>, <em>Holder’s</em>, <em>triangle inequality</em></p></li>
<li><p>Identifiability proof simple enough to be phrased in this proof system <span class="math inline">\(\rightarrow\)</span> near-integrality of clustering SDP</p></li>
</ol>
</div>
</section>
<section class="slide level1">

<p>Any subset of <span class="math inline">\(n/k\)</span> samples w. bounded <span class="math inline">\(1/\e\)</span> moments is nearly a true cluster (whose mean is nearly a <span class="math inline">\(\mu_i\)</span>). <span class="magenta"><strong>Proof using only Holder and triangle ineq’s:</strong></span><br />
<span class="green">Recall <span class="math inline">\(\Delta = k^\epsilon\)</span></span></p>
<p>Since <em>true</em> clusters are <em>subgaussian</em> – <span class="math inline">\((10/\e)\)</span>-th moment is at most <span class="math inline">\(O_\e(1)\)</span> in every direction.</p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="proof-by-picture-small.png" /></p>
</div><div class="column" style="width:40%;">
<p><span class="red">red</span> = putative cluster <span class="red"><span class="math inline">\(S\)</span></span></p>
<p><span class="fragment">If a <span class="math inline">\(1/k^2\)</span>-fraction of <span class="red"><span class="math inline">\(S\)</span></span> is in cluster <span class="math inline">\(2\)</span></span></p>
<p><span class="fragment">then <span class="math inline">\(10/\e\)</span>-th empirical moment <span class="blue"><span class="math inline">\(\frac 1 {|S|} \sum_{i \in S} \langle x_i - \mu(S), \mu_2 - \mu(S) \rangle^{10/\e}\)</span></span> of <span class="red"><span class="math inline">\(S\)</span></span> is at least <span class="math display">\[
\frac 1 {k^2} \cdot [\Omega(\Delta)]^{10/\e} \geq k^8 &gt;&gt; O_\e(1)
\]</span></span></p>
</div>
</div>
</section>
<section class="slide level1">

<h2 id="recap">Recap</h2>
<p>New algorithm design technique, using Sum of Squares SDPs, for moment estimation when not all data comes from <span class="math inline">\(\cD\)</span>.</p>
<p>First improvement in separation for <strong>Gaussian mixtures models</strong> in 15 years.</p>
<p>Automatic robustification of moment-method algorithms.</p>
</section>
<section id="thanks-questions" class="slide level1">
<h1>Thanks! Questions?</h1>
</section>
<section class="slide level1">

<h2 id="high-dimensional-estimation-tasks">High-Dimensional Estimation Tasks</h2>
<p><strong>Parameters:</strong> <span class="math inline">\(\theta \in \R^K\)</span><br />
<strong>Samples:</strong> <span class="math inline">\(X_1,\ldots,X_n \sim \P(X \, | \, \theta)\)</span> with <span class="math inline">\(X_i \in \R^d\)</span></p>
<div class="fragment">
<p><strong>Goal:</strong> estimate <span class="math inline">\(\theta\)</span> by <span class="math inline">\(\widehat{\theta}(X_1,\ldots,X_n)\)</span> <em>via computationally-efficient algorithm</em>.</p>
<p><strong>Challenge:</strong> traditional statistical approaches (max-likelihood, etc.) often exponential in (at least) one of <span class="math inline">\(d,K,n\)</span>.</p>
</div>
<div class="fragment">
<p><strong>Example (known in ancient Greece, India, etc.):</strong> estimate <span class="math inline">\(\mu \in \R^d\)</span> from <span class="math inline">\(X_1,\ldots,X_n \sim \cD\)</span>, mean of <span class="math inline">\(\cD\)</span> is <span class="math inline">\(\mu\)</span> <span class="blue">(use empirical mean)</span></p>
</div>
<div class="fragment">
<p><strong>Example (Pearson, 1890s):</strong> estimate <span class="math inline">\(\mu_1,\ldots,\mu_k \in \R^d\)</span> from <span class="math inline">\(X_1,\ldots,X_n \sim \frac 1k \sum_{i \in [k]} \cD_i\)</span>, mean of <span class="math inline">\(\cD_i\)</span> is <span class="math inline">\(\mu_i\)</span>.<br />
<span class="blue">(clustering, moment methods, dimension reduction, …)</span></p>
</div>
<h2 id="robust-high-dimensional-estimation-tasks">Robust High-Dimensional Estimation Tasks</h2>
<p><strong>Parameters:</strong> <span class="math inline">\(\theta \in \R^K\)</span><br />
<strong>Samples:</strong> <span class="math inline">\(X_1,\ldots,X_n \sim \P(X \, | \, \theta)\)</span> with <span class="math inline">\(X_i \in \R^d\)</span></p>
<p><strong>Adversary:</strong> replace <span class="math inline">\(X_1,\ldots,X_n\)</span> with <span class="math inline">\(Y_1,\ldots,Y_n\)</span>, with <span class="math inline">\(Y_i = X_i\)</span> for at least <span class="math inline">\((1-\e)n\)</span> samples</p>
<div class="fragment">
<p><strong>Goal:</strong> estimate <span class="math inline">\(\theta\)</span> by <span class="math inline">\(\widehat{\theta}(Y_1,\ldots,Y_n)\)</span> <em>via computationally-efficient algorithm</em>.</p>
<p><strong>Challenge:</strong> even for very simple estimation tasks like <span class="blue"><em>mean estimation</em></span>, the obvious estimators/algorithms incur <em>dimension-dependent error</em>.</p>
</div>
<div class="fragment">
<p><strong>Example:</strong> estimate mean of <span class="math inline">\(\cN(\mu, I)\)</span> from <span class="math inline">\(\e\)</span>-corrputed samples<br />
<span class="blue">after removing “obvious outliers”, empirical mean still poor: adversary can achieve <span class="math inline">\(\|\widehat{\mu} - \mu\| \approx \e \sqrt d\)</span></span></p>
<p>First <em>poly time, dimension-independent</em> guarantees very recent [DKK+16, LRV16].</p>
</div>
<div class="fragment">
<p><strong>Example (Pearson, 1890s):</strong> estimate <span class="math inline">\(\mu_1,\ldots,\mu_k \in \R^d\)</span> from <span class="math inline">\(X_1,\ldots,X_n \sim \frac 1k \sum_{i \in [k]} \cD_i\)</span>, mean of <span class="math inline">\(\cD_i\)</span> is <span class="math inline">\(\mu_i\)</span>.<br />
<span class="blue">(clustering, moment methods, dimension reduction, …)</span></p>
<p><span class="magenta center"><strong>complex estimation tasks only get harder</strong></span></p>
</div>
</section>
<section class="slide level1">

<h2 id="main-contribution">Main Contribution</h2>
<p><span class="center"><strong>A new algorithm-design technique for high-dimensional estimation.</strong></span></p>
<div class="fragment">
<blockquote>
<p><strong>New algorithms for:</strong></p>
<ul>
<li>clustering, learning mixture models</li>
<li>robust mean estimation</li>
<li>robust higher-moment estimation</li>
<li>robust independent component analysis</li>
<li>list-decodable learning</li>
</ul>
</blockquote>
</div>
<div class="fragment">
<blockquote>
<p><strong>Estimate empirical mean/moments of a “good” subset of samples.</strong><br />
<em>Robust estimation:</em> <span class="blue">good subset = uncorrupted samples</span><br />
<em>Clustering/mixture models:</em> <span class="blue">good subset = a single cluster</span></p>
</blockquote>
</div>
<div class="fragment">
<p>Uses the <em>sum of squares method (SoS)</em> to turn <span class="magenta">simple identifiability proofs into efficient semidefinite-programming-based algorithms</span>.</p>
</div>
</section>
<section class="slide level1">

<h2 id="agenda-1">Agenda</h2>
<ol type="1">
<li>Overview of results</li>
<li>Zoom in on simple case – <strong>well-separated mixtures of Gaussians</strong></li>
</ol>
</section>
<section class="slide level1">

<blockquote>
<p><strong>Mixture models:</strong> Can cluster and estimate means of a mixture of <span class="math inline">\(k\)</span> <span class="blue"><strong>unknown</strong></span> <span class="math inline">\(d\)</span>-dimensional distributions <span class="math inline">\(\cD_i\)</span> with <span class="math inline">\(\poly(k,d)\)</span> time/samples, so long as</p>
<p><em>Separation:</em> <span class="math inline">\(\|\mu_i - \mu_j\| \geq k^{\e}\)</span><br />
<em>Bounded moments:</em> <span class="math inline">\(\cD_i\)</span> has <span class="math inline">\(O(1/\e)\)</span> subgaussian moments, and this has a simple (SoS) proof</p>
<p><span class="magenta">ex: strongly <span class="math inline">\(\log\)</span>-concave distributions, rotations product distributions with <span class="math inline">\(O(1/\e)\)</span> subgaussian moments</span></p>
<p>First tight guarantees for distributions with bounded <span class="math inline">\(O(1/\e)\)</span> moments; <strong>approaches information-theoretic optimality for Gaussian mixtures.</strong></p>
</blockquote>
</section>
<section class="slide level1">

<p><strong>Moment tensor:</strong> <span class="math inline">\(r\)</span>-th moment <span class="math inline">\(M_r\)</span> of a distribution <span class="math inline">\(\cD\)</span> has entries <span class="math inline">\((M_r)_{i_1,\ldots,i_r} = \E_{X \sim \cD} X_{i_1} \cdot \ldots \cdot X_{i_r}\)</span>.<br />
<span class="blue">(<span class="math inline">\(r=2\)</span> is the covariance matrix)</span></p>
<div class="fragment">
<blockquote>
<p><strong>Robust moment estimation:</strong> Given an <span class="math inline">\(\e\)</span>-corrupted set of <span class="math inline">\(d^{O(t)}\)</span> samples from unknown <span class="math inline">\(d\)</span>-dimensional distribution <span class="math inline">\(\cD\)</span>, can estimate <span class="math inline">\(r\)</span>-th moment tensor <span class="math inline">\(M_r\)</span> of <span class="math inline">\(\cD\)</span> in <em>injective tensor norm</em> (high-order analogue of spectral norm)</p>
<p><span class="math display">\[\max_{u \in \R^d} \langle \widehat{M_r} - M_r, u^{\otimes r} \rangle \leq \e^{1-\tfrac r t} \cdot O(t^{r/2})\]</span></p>
<p>if <span class="math inline">\(\cD\)</span> has bounded <span class="math inline">\(t\)</span>-th moments and this has a simple (SoS) proof.</p>
<p><strong>First dimension-independent guarantees for robust higher-moment estimation beyond Gaussians</strong></p>
</blockquote>
</div>
</section>
<section class="slide level1">

<blockquote>
<p><strong>Application: Robust Independent Component Analysis</strong></p>
<p><strong>Parameters:</strong> <span class="math inline">\(a_1,\ldots,a_d \in \R^d\)</span>. Let <span class="math inline">\(A = (a_1,\ldots, a_d)\)</span> nonsingular<br />
<strong>Samples:</strong> <span class="math inline">\(X = AY\)</span> with <span class="math inline">\(Y \sim \cD\)</span> for unknown, non-Gaussian <span class="math inline">\(\cD\)</span></p>
<p><strong>Goal:</strong> estimate <span class="math inline">\(a_1,\ldots,a_n\)</span></p>
<p><strong>Classic alg</strong> (orthogonal case): estimate <span class="math inline">\(\E X^{\otimes 4} \approx \sum_{i \in [n]} a_i^{\otimes 4}\)</span> from samples, apply tensor decomposition</p>
<p><span class="blue"><strong>Robust <span class="math inline">\(4\)</span>-th moment estimation automatically robust-ifies this algorithm</strong></span></p>
</blockquote>
</section>
<section class="slide level1">

<h2 id="mixture-models-3">Mixture Models</h2>
<p><strong>Parameters:</strong> cluster centers <span class="math inline">\(\mu_1,\ldots,\mu_k \in \R^d\)</span><br />
<strong>Samples:</strong> <span class="math inline">\(X_1,\ldots,X_n \sim \frac 1k \sum_{i \leq k} \cD_i\)</span> where <span class="math inline">\(\E_{X \sim \cD_i} = \mu_i\)</span>.</p>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: false,
        // Display a presentation progress bar
        progress: false,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: false,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
